{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.96,
  "eval_steps": 500,
  "global_step": 37000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 4.036301136016846,
      "learning_rate": 5.226666666666667e-06,
      "loss": 2.9188,
      "step": 50
    },
    {
      "epoch": 0.008,
      "grad_norm": 4.327671527862549,
      "learning_rate": 1.056e-05,
      "loss": 2.0001,
      "step": 100
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.0353145599365234,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 1.079,
      "step": 150
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.143392324447632,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.8567,
      "step": 200
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.0781452655792236,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.8809,
      "step": 250
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.3200180530548096,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.8153,
      "step": 300
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.961196780204773,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.8805,
      "step": 350
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.563934326171875,
      "learning_rate": 4.256e-05,
      "loss": 0.8083,
      "step": 400
    },
    {
      "epoch": 0.036,
      "grad_norm": 1.2312133312225342,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.792,
      "step": 450
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.197817325592041,
      "learning_rate": 5.322666666666667e-05,
      "loss": 0.8011,
      "step": 500
    },
    {
      "epoch": 0.044,
      "grad_norm": 1.155632495880127,
      "learning_rate": 5.856e-05,
      "loss": 0.7727,
      "step": 550
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.529426634311676,
      "learning_rate": 6.389333333333334e-05,
      "loss": 0.8304,
      "step": 600
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.9489085674285889,
      "learning_rate": 6.922666666666667e-05,
      "loss": 0.7749,
      "step": 650
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.7176479697227478,
      "learning_rate": 7.456e-05,
      "loss": 0.7623,
      "step": 700
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.8308932185173035,
      "learning_rate": 7.989333333333334e-05,
      "loss": 0.8348,
      "step": 750
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.5124819874763489,
      "learning_rate": 8.522666666666667e-05,
      "loss": 0.7622,
      "step": 800
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.8811258673667908,
      "learning_rate": 9.056e-05,
      "loss": 0.785,
      "step": 850
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.2727378606796265,
      "learning_rate": 9.589333333333333e-05,
      "loss": 0.809,
      "step": 900
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.39080339670181274,
      "learning_rate": 0.00010122666666666666,
      "loss": 0.7372,
      "step": 950
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.9841852784156799,
      "learning_rate": 0.00010656000000000001,
      "loss": 0.7778,
      "step": 1000
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.9062634110450745,
      "learning_rate": 0.00011189333333333334,
      "loss": 0.8184,
      "step": 1050
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.7768207788467407,
      "learning_rate": 0.00011722666666666666,
      "loss": 0.8593,
      "step": 1100
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.957435667514801,
      "learning_rate": 0.00012256000000000002,
      "loss": 0.7669,
      "step": 1150
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.7473271489143372,
      "learning_rate": 0.00012789333333333334,
      "loss": 0.777,
      "step": 1200
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.1652112007141113,
      "learning_rate": 0.00013322666666666668,
      "loss": 0.8301,
      "step": 1250
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.2865859866142273,
      "learning_rate": 0.00013856,
      "loss": 0.7788,
      "step": 1300
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.3499455749988556,
      "learning_rate": 0.00014389333333333335,
      "loss": 0.7672,
      "step": 1350
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.4750247597694397,
      "learning_rate": 0.00014922666666666667,
      "loss": 0.826,
      "step": 1400
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.8283874988555908,
      "learning_rate": 0.00015456,
      "loss": 0.8018,
      "step": 1450
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.4409337043762207,
      "learning_rate": 0.00015989333333333333,
      "loss": 0.8977,
      "step": 1500
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.3855147063732147,
      "learning_rate": 0.00016522666666666667,
      "loss": 0.7352,
      "step": 1550
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5775200724601746,
      "learning_rate": 0.00017056000000000002,
      "loss": 0.8514,
      "step": 1600
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.7434160113334656,
      "learning_rate": 0.00017589333333333334,
      "loss": 0.8467,
      "step": 1650
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.9836111068725586,
      "learning_rate": 0.00018122666666666668,
      "loss": 0.7996,
      "step": 1700
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.1337380409240723,
      "learning_rate": 0.00018656,
      "loss": 0.7773,
      "step": 1750
    },
    {
      "epoch": 0.144,
      "grad_norm": 1.0539517402648926,
      "learning_rate": 0.00019189333333333335,
      "loss": 0.6891,
      "step": 1800
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.7639974355697632,
      "learning_rate": 0.00019722666666666667,
      "loss": 0.8468,
      "step": 1850
    },
    {
      "epoch": 0.152,
      "grad_norm": 1.4259804487228394,
      "learning_rate": 0.0001999997760338807,
      "loss": 0.8034,
      "step": 1900
    },
    {
      "epoch": 0.156,
      "grad_norm": 1.1378438472747803,
      "learning_rate": 0.0001999978707733075,
      "loss": 0.7674,
      "step": 1950
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.3627418279647827,
      "learning_rate": 0.00019999402140621236,
      "loss": 0.7979,
      "step": 2000
    },
    {
      "epoch": 0.164,
      "grad_norm": 1.0644587278366089,
      "learning_rate": 0.0001999882280074327,
      "loss": 0.8327,
      "step": 2050
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.8143661618232727,
      "learning_rate": 0.00019998049068960073,
      "loss": 0.7514,
      "step": 2100
    },
    {
      "epoch": 0.172,
      "grad_norm": 1.089315414428711,
      "learning_rate": 0.00019997080960314142,
      "loss": 0.7936,
      "step": 2150
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.3433783948421478,
      "learning_rate": 0.00019995918493626933,
      "loss": 0.7252,
      "step": 2200
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.33522331714630127,
      "learning_rate": 0.0001999456169149852,
      "loss": 0.8475,
      "step": 2250
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.7284100651741028,
      "learning_rate": 0.00019993010580307148,
      "loss": 0.7993,
      "step": 2300
    },
    {
      "epoch": 0.188,
      "grad_norm": 3.1985323429107666,
      "learning_rate": 0.0001999126519020871,
      "loss": 0.7783,
      "step": 2350
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.1958576440811157,
      "learning_rate": 0.00019989325555136168,
      "loss": 0.8103,
      "step": 2400
    },
    {
      "epoch": 0.196,
      "grad_norm": 1.4172799587249756,
      "learning_rate": 0.00019987191712798902,
      "loss": 0.818,
      "step": 2450
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.348247766494751,
      "learning_rate": 0.0001998486370468196,
      "loss": 0.8147,
      "step": 2500
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.3670700788497925,
      "learning_rate": 0.00019982341576045264,
      "loss": 0.7922,
      "step": 2550
    },
    {
      "epoch": 0.208,
      "grad_norm": 1.5391510725021362,
      "learning_rate": 0.00019979625375922728,
      "loss": 0.851,
      "step": 2600
    },
    {
      "epoch": 0.212,
      "grad_norm": 1.9322843551635742,
      "learning_rate": 0.00019976715157121296,
      "loss": 0.8153,
      "step": 2650
    },
    {
      "epoch": 0.216,
      "grad_norm": 1.0259135961532593,
      "learning_rate": 0.0001997361097621993,
      "loss": 0.7775,
      "step": 2700
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.6253024339675903,
      "learning_rate": 0.00019970312893568496,
      "loss": 0.8795,
      "step": 2750
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.073486566543579,
      "learning_rate": 0.000199668209732866,
      "loss": 0.7982,
      "step": 2800
    },
    {
      "epoch": 0.228,
      "grad_norm": 1.4875081777572632,
      "learning_rate": 0.0001996313528326234,
      "loss": 0.8298,
      "step": 2850
    },
    {
      "epoch": 0.232,
      "grad_norm": 1.2955151796340942,
      "learning_rate": 0.0001995925589515097,
      "loss": 0.8324,
      "step": 2900
    },
    {
      "epoch": 0.236,
      "grad_norm": 1.6036171913146973,
      "learning_rate": 0.0001995518288437355,
      "loss": 0.8172,
      "step": 2950
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.24211473762989044,
      "learning_rate": 0.00019950916330115423,
      "loss": 0.7457,
      "step": 3000
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.023396372795105,
      "learning_rate": 0.0001994645631532472,
      "loss": 0.7793,
      "step": 3050
    },
    {
      "epoch": 0.248,
      "grad_norm": 1.2300188541412354,
      "learning_rate": 0.0001994180292671072,
      "loss": 0.8246,
      "step": 3100
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.22452294826507568,
      "learning_rate": 0.0001993695625474219,
      "loss": 0.8131,
      "step": 3150
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.242655411362648,
      "learning_rate": 0.0001993191639364559,
      "loss": 0.7801,
      "step": 3200
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.7709405422210693,
      "learning_rate": 0.00019926683441403288,
      "loss": 0.8658,
      "step": 3250
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.8819804787635803,
      "learning_rate": 0.00019921257499751603,
      "loss": 0.7721,
      "step": 3300
    },
    {
      "epoch": 0.268,
      "grad_norm": 1.4831444025039673,
      "learning_rate": 0.0001991563867417888,
      "loss": 0.7679,
      "step": 3350
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.34081926941871643,
      "learning_rate": 0.00019909827073923389,
      "loss": 0.8286,
      "step": 3400
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.9950483441352844,
      "learning_rate": 0.00019903822811971237,
      "loss": 0.7577,
      "step": 3450
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.5719730854034424,
      "learning_rate": 0.0001989762600505416,
      "loss": 0.8163,
      "step": 3500
    },
    {
      "epoch": 0.284,
      "grad_norm": 1.43880033493042,
      "learning_rate": 0.0001989123677364725,
      "loss": 0.7329,
      "step": 3550
    },
    {
      "epoch": 0.288,
      "grad_norm": 1.0760294198989868,
      "learning_rate": 0.00019884655241966616,
      "loss": 0.7216,
      "step": 3600
    },
    {
      "epoch": 0.292,
      "grad_norm": 1.1171940565109253,
      "learning_rate": 0.0001987788153796697,
      "loss": 0.8408,
      "step": 3650
    },
    {
      "epoch": 0.296,
      "grad_norm": 1.249164342880249,
      "learning_rate": 0.00019870915793339137,
      "loss": 0.8041,
      "step": 3700
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.6557212471961975,
      "learning_rate": 0.00019863758143507493,
      "loss": 0.821,
      "step": 3750
    },
    {
      "epoch": 0.304,
      "grad_norm": 1.4069228172302246,
      "learning_rate": 0.00019856408727627343,
      "loss": 0.7127,
      "step": 3800
    },
    {
      "epoch": 0.308,
      "grad_norm": 1.4046337604522705,
      "learning_rate": 0.00019848867688582197,
      "loss": 0.8041,
      "step": 3850
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.6849828958511353,
      "learning_rate": 0.00019841135172981016,
      "loss": 0.859,
      "step": 3900
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.8173682689666748,
      "learning_rate": 0.00019833211331155335,
      "loss": 0.8159,
      "step": 3950
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.6085737347602844,
      "learning_rate": 0.00019825096317156363,
      "loss": 0.7816,
      "step": 4000
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.732604742050171,
      "learning_rate": 0.00019816790288751976,
      "loss": 0.829,
      "step": 4050
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.34845712780952454,
      "learning_rate": 0.00019808293407423647,
      "loss": 0.8709,
      "step": 4100
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.0095144510269165,
      "learning_rate": 0.00019799605838363325,
      "loss": 0.8551,
      "step": 4150
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.32228726148605347,
      "learning_rate": 0.00019790727750470196,
      "loss": 0.813,
      "step": 4200
    },
    {
      "epoch": 0.34,
      "grad_norm": 2.385441780090332,
      "learning_rate": 0.00019781659316347425,
      "loss": 0.8429,
      "step": 4250
    },
    {
      "epoch": 0.344,
      "grad_norm": 1.431747317314148,
      "learning_rate": 0.0001977240071229878,
      "loss": 0.8614,
      "step": 4300
    },
    {
      "epoch": 0.348,
      "grad_norm": 1.2998706102371216,
      "learning_rate": 0.00019762952118325232,
      "loss": 0.8528,
      "step": 4350
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.9209871888160706,
      "learning_rate": 0.00019753313718121407,
      "loss": 0.8101,
      "step": 4400
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.3474179804325104,
      "learning_rate": 0.00019743485699072073,
      "loss": 0.7594,
      "step": 4450
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.7572401762008667,
      "learning_rate": 0.00019733468252248454,
      "loss": 0.8556,
      "step": 4500
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.435676634311676,
      "learning_rate": 0.0001972326157240453,
      "loss": 0.7641,
      "step": 4550
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.2738579511642456,
      "learning_rate": 0.00019712865857973263,
      "loss": 0.8069,
      "step": 4600
    },
    {
      "epoch": 0.372,
      "grad_norm": 1.3839573860168457,
      "learning_rate": 0.00019702281311062713,
      "loss": 0.9207,
      "step": 4650
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.2691028118133545,
      "learning_rate": 0.0001969150813745213,
      "loss": 0.7781,
      "step": 4700
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.7920876741409302,
      "learning_rate": 0.00019680546546587945,
      "loss": 0.8706,
      "step": 4750
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.276539921760559,
      "learning_rate": 0.00019669396751579693,
      "loss": 0.8398,
      "step": 4800
    },
    {
      "epoch": 0.388,
      "grad_norm": 1.0069040060043335,
      "learning_rate": 0.00019658058969195888,
      "loss": 0.8222,
      "step": 4850
    },
    {
      "epoch": 0.392,
      "grad_norm": 1.2565115690231323,
      "learning_rate": 0.00019646533419859788,
      "loss": 0.738,
      "step": 4900
    },
    {
      "epoch": 0.396,
      "grad_norm": 1.4918614625930786,
      "learning_rate": 0.0001963482032764512,
      "loss": 0.7583,
      "step": 4950
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.37579432129859924,
      "learning_rate": 0.00019622919920271725,
      "loss": 0.7912,
      "step": 5000
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.9693179726600647,
      "learning_rate": 0.0001961083242910112,
      "loss": 0.7792,
      "step": 5050
    },
    {
      "epoch": 0.408,
      "grad_norm": 1.5217385292053223,
      "learning_rate": 0.00019598558089132015,
      "loss": 0.782,
      "step": 5100
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.2753170728683472,
      "learning_rate": 0.0001958609713899574,
      "loss": 0.7977,
      "step": 5150
    },
    {
      "epoch": 0.416,
      "grad_norm": 1.3089998960494995,
      "learning_rate": 0.00019573449820951586,
      "loss": 0.795,
      "step": 5200
    },
    {
      "epoch": 0.42,
      "grad_norm": 2.0561842918395996,
      "learning_rate": 0.00019560616380882135,
      "loss": 0.7772,
      "step": 5250
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.2079163789749146,
      "learning_rate": 0.00019547597068288438,
      "loss": 0.8361,
      "step": 5300
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.22363436222076416,
      "learning_rate": 0.00019534392136285193,
      "loss": 0.8198,
      "step": 5350
    },
    {
      "epoch": 0.432,
      "grad_norm": 1.5879647731781006,
      "learning_rate": 0.0001952100184159581,
      "loss": 0.8096,
      "step": 5400
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.2245815545320511,
      "learning_rate": 0.0001950742644454742,
      "loss": 0.7968,
      "step": 5450
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.3849292993545532,
      "learning_rate": 0.0001949366620906583,
      "loss": 0.8381,
      "step": 5500
    },
    {
      "epoch": 0.444,
      "grad_norm": 1.9906337261199951,
      "learning_rate": 0.00019479721402670364,
      "loss": 0.7741,
      "step": 5550
    },
    {
      "epoch": 0.448,
      "grad_norm": 1.394304871559143,
      "learning_rate": 0.0001946559229646869,
      "loss": 0.8447,
      "step": 5600
    },
    {
      "epoch": 0.452,
      "grad_norm": 1.4190863370895386,
      "learning_rate": 0.00019451279165151527,
      "loss": 0.7651,
      "step": 5650
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.31534910202026367,
      "learning_rate": 0.00019436782286987324,
      "loss": 0.8013,
      "step": 5700
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.5832409858703613,
      "learning_rate": 0.00019422101943816834,
      "loss": 0.7844,
      "step": 5750
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.3385661542415619,
      "learning_rate": 0.00019407238421047647,
      "loss": 0.788,
      "step": 5800
    },
    {
      "epoch": 0.468,
      "grad_norm": 1.2779220342636108,
      "learning_rate": 0.00019392192007648625,
      "loss": 0.7467,
      "step": 5850
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.916534423828125,
      "learning_rate": 0.000193769629961443,
      "loss": 0.7891,
      "step": 5900
    },
    {
      "epoch": 0.476,
      "grad_norm": 2.8429956436157227,
      "learning_rate": 0.0001936155168260919,
      "loss": 0.8399,
      "step": 5950
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.408491611480713,
      "learning_rate": 0.0001934595836666202,
      "loss": 0.8741,
      "step": 6000
    },
    {
      "epoch": 0.484,
      "grad_norm": 1.1211847066879272,
      "learning_rate": 0.00019330183351459922,
      "loss": 0.7589,
      "step": 6050
    },
    {
      "epoch": 0.488,
      "grad_norm": 1.4576457738876343,
      "learning_rate": 0.00019314226943692515,
      "loss": 0.8166,
      "step": 6100
    },
    {
      "epoch": 0.492,
      "grad_norm": 1.9408677816390991,
      "learning_rate": 0.0001929808945357598,
      "loss": 0.7918,
      "step": 6150
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.47621840238571167,
      "learning_rate": 0.0001928177119484699,
      "loss": 0.8356,
      "step": 6200
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.33246517181396484,
      "learning_rate": 0.00019265272484756633,
      "loss": 0.797,
      "step": 6250
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.6392743587493896,
      "learning_rate": 0.0001924859364406424,
      "loss": 0.7959,
      "step": 6300
    },
    {
      "epoch": 0.508,
      "grad_norm": 1.2758995294570923,
      "learning_rate": 0.00019231734997031137,
      "loss": 0.7796,
      "step": 6350
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.4270906448364258,
      "learning_rate": 0.00019214696871414365,
      "loss": 0.8323,
      "step": 6400
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.25012826919555664,
      "learning_rate": 0.0001919747959846029,
      "loss": 0.8245,
      "step": 6450
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.1186589002609253,
      "learning_rate": 0.00019180083512898165,
      "loss": 0.8342,
      "step": 6500
    },
    {
      "epoch": 0.524,
      "grad_norm": 1.225247859954834,
      "learning_rate": 0.0001916250895293362,
      "loss": 0.785,
      "step": 6550
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.3355454206466675,
      "learning_rate": 0.00019144756260242097,
      "loss": 0.7714,
      "step": 6600
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.4756684303283691,
      "learning_rate": 0.0001912682577996221,
      "loss": 0.8186,
      "step": 6650
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.5311588048934937,
      "learning_rate": 0.0001910871786068901,
      "loss": 0.8292,
      "step": 6700
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.6728765964508057,
      "learning_rate": 0.00019090432854467233,
      "loss": 0.8288,
      "step": 6750
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.7188188433647156,
      "learning_rate": 0.0001907197111678445,
      "loss": 0.8319,
      "step": 6800
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.5159667134284973,
      "learning_rate": 0.00019053333006564146,
      "loss": 0.6894,
      "step": 6850
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.9934784173965454,
      "learning_rate": 0.00019034518886158758,
      "loss": 0.7354,
      "step": 6900
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.4800281524658203,
      "learning_rate": 0.00019015529121342608,
      "loss": 0.8024,
      "step": 6950
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.6765647530555725,
      "learning_rate": 0.0001899636408130482,
      "loss": 0.8574,
      "step": 7000
    },
    {
      "epoch": 0.564,
      "grad_norm": 1.6124591827392578,
      "learning_rate": 0.00018977024138642115,
      "loss": 0.7646,
      "step": 7050
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.682236671447754,
      "learning_rate": 0.00018957509669351595,
      "loss": 0.7484,
      "step": 7100
    },
    {
      "epoch": 0.572,
      "grad_norm": 1.321763038635254,
      "learning_rate": 0.00018937821052823398,
      "loss": 0.8149,
      "step": 7150
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.4820747375488281,
      "learning_rate": 0.00018917958671833354,
      "loss": 0.8237,
      "step": 7200
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.2610441446304321,
      "learning_rate": 0.00018897922912535536,
      "loss": 0.7094,
      "step": 7250
    },
    {
      "epoch": 0.584,
      "grad_norm": 1.2508703470230103,
      "learning_rate": 0.00018877714164454735,
      "loss": 0.7873,
      "step": 7300
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.20849943161010742,
      "learning_rate": 0.00018857332820478907,
      "loss": 0.7885,
      "step": 7350
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.24792858958244324,
      "learning_rate": 0.00018836779276851527,
      "loss": 0.764,
      "step": 7400
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.4951834678649902,
      "learning_rate": 0.00018816053933163885,
      "loss": 0.7765,
      "step": 7450
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.5506480932235718,
      "learning_rate": 0.00018795157192347318,
      "loss": 0.7646,
      "step": 7500
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.27698996663093567,
      "learning_rate": 0.0001877408946066538,
      "loss": 0.8728,
      "step": 7550
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.3942608833312988,
      "learning_rate": 0.00018752851147705934,
      "loss": 0.9127,
      "step": 7600
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.4128698706626892,
      "learning_rate": 0.00018731442666373204,
      "loss": 0.7724,
      "step": 7650
    },
    {
      "epoch": 0.616,
      "grad_norm": 1.0682908296585083,
      "learning_rate": 0.00018709864432879728,
      "loss": 0.8239,
      "step": 7700
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.7720909714698792,
      "learning_rate": 0.0001868811686673829,
      "loss": 0.7529,
      "step": 7750
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.3170665502548218,
      "learning_rate": 0.00018666200390753736,
      "loss": 0.8391,
      "step": 7800
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.2676061689853668,
      "learning_rate": 0.00018644115431014783,
      "loss": 0.7352,
      "step": 7850
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.8522605895996094,
      "learning_rate": 0.00018621862416885713,
      "loss": 0.8508,
      "step": 7900
    },
    {
      "epoch": 0.636,
      "grad_norm": 1.324686050415039,
      "learning_rate": 0.00018599441780998041,
      "loss": 0.8255,
      "step": 7950
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.1758400201797485,
      "learning_rate": 0.00018576853959242088,
      "loss": 0.8065,
      "step": 8000
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.9447606205940247,
      "learning_rate": 0.00018554099390758524,
      "loss": 0.7824,
      "step": 8050
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.5229988098144531,
      "learning_rate": 0.00018531178517929811,
      "loss": 0.8289,
      "step": 8100
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.4439365565776825,
      "learning_rate": 0.00018508091786371633,
      "loss": 0.8337,
      "step": 8150
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.9773839712142944,
      "learning_rate": 0.00018484839644924191,
      "loss": 0.7523,
      "step": 8200
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.4665985107421875,
      "learning_rate": 0.0001846142254564351,
      "loss": 0.7573,
      "step": 8250
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.2811567783355713,
      "learning_rate": 0.0001843784094379264,
      "loss": 0.8082,
      "step": 8300
    },
    {
      "epoch": 0.668,
      "grad_norm": 1.7221927642822266,
      "learning_rate": 0.00018414095297832801,
      "loss": 0.7577,
      "step": 8350
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.3786206841468811,
      "learning_rate": 0.0001839018606941447,
      "loss": 0.8078,
      "step": 8400
    },
    {
      "epoch": 0.676,
      "grad_norm": 1.4946919679641724,
      "learning_rate": 0.00018366113723368427,
      "loss": 0.793,
      "step": 8450
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.4994584023952484,
      "learning_rate": 0.00018341878727696673,
      "loss": 0.8651,
      "step": 8500
    },
    {
      "epoch": 0.684,
      "grad_norm": 1.0696524381637573,
      "learning_rate": 0.0001831748155356338,
      "loss": 0.8227,
      "step": 8550
    },
    {
      "epoch": 0.688,
      "grad_norm": 1.8054670095443726,
      "learning_rate": 0.0001829292267528569,
      "loss": 0.8526,
      "step": 8600
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.3151704668998718,
      "learning_rate": 0.00018268202570324534,
      "loss": 0.7634,
      "step": 8650
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.3825765550136566,
      "learning_rate": 0.00018243321719275314,
      "loss": 0.8518,
      "step": 8700
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.0420732498168945,
      "learning_rate": 0.00018218280605858576,
      "loss": 0.821,
      "step": 8750
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.1034830808639526,
      "learning_rate": 0.00018193079716910608,
      "loss": 0.8257,
      "step": 8800
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.3980530500411987,
      "learning_rate": 0.00018167719542373968,
      "loss": 0.8618,
      "step": 8850
    },
    {
      "epoch": 0.712,
      "grad_norm": 2.0498499870300293,
      "learning_rate": 0.00018142200575287958,
      "loss": 0.755,
      "step": 8900
    },
    {
      "epoch": 0.716,
      "grad_norm": 1.63344144821167,
      "learning_rate": 0.00018116523311779043,
      "loss": 0.7885,
      "step": 8950
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.8431605100631714,
      "learning_rate": 0.00018090688251051214,
      "loss": 0.8214,
      "step": 9000
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.333360880613327,
      "learning_rate": 0.00018064695895376263,
      "loss": 0.7666,
      "step": 9050
    },
    {
      "epoch": 0.728,
      "grad_norm": 1.3917772769927979,
      "learning_rate": 0.00018038546750084026,
      "loss": 0.859,
      "step": 9100
    },
    {
      "epoch": 0.732,
      "grad_norm": 2.0287845134735107,
      "learning_rate": 0.00018012241323552575,
      "loss": 0.7409,
      "step": 9150
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.1750621795654297,
      "learning_rate": 0.00017985780127198307,
      "loss": 0.8303,
      "step": 9200
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.34353506565094,
      "learning_rate": 0.00017959163675466017,
      "loss": 0.8296,
      "step": 9250
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.2770432233810425,
      "learning_rate": 0.00017932392485818906,
      "loss": 0.8866,
      "step": 9300
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.282833456993103,
      "learning_rate": 0.00017905467078728493,
      "loss": 0.8118,
      "step": 9350
    },
    {
      "epoch": 0.752,
      "grad_norm": 1.5688838958740234,
      "learning_rate": 0.0001787838797766452,
      "loss": 0.7973,
      "step": 9400
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.4890510141849518,
      "learning_rate": 0.0001785115570908477,
      "loss": 0.8002,
      "step": 9450
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.5580194592475891,
      "learning_rate": 0.0001782377080242482,
      "loss": 0.8172,
      "step": 9500
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.8643090128898621,
      "learning_rate": 0.00017796233790087768,
      "loss": 0.8195,
      "step": 9550
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.6887948513031006,
      "learning_rate": 0.00017768545207433862,
      "loss": 0.8094,
      "step": 9600
    },
    {
      "epoch": 0.772,
      "grad_norm": 1.2409108877182007,
      "learning_rate": 0.00017740705592770106,
      "loss": 0.8648,
      "step": 9650
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.3842169940471649,
      "learning_rate": 0.0001771271548733979,
      "loss": 0.8615,
      "step": 9700
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.5867722630500793,
      "learning_rate": 0.00017684575435311962,
      "loss": 0.7821,
      "step": 9750
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.4636669456958771,
      "learning_rate": 0.0001765628598377086,
      "loss": 0.7445,
      "step": 9800
    },
    {
      "epoch": 0.788,
      "grad_norm": 1.1283175945281982,
      "learning_rate": 0.0001762784768270527,
      "loss": 0.7761,
      "step": 9850
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.4594587981700897,
      "learning_rate": 0.00017599261084997824,
      "loss": 0.8577,
      "step": 9900
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.302777647972107,
      "learning_rate": 0.0001757052674641427,
      "loss": 0.7576,
      "step": 9950
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.0659937858581543,
      "learning_rate": 0.00017541645225592652,
      "loss": 0.8341,
      "step": 10000
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.8790109753608704,
      "learning_rate": 0.00017512617084032453,
      "loss": 0.8382,
      "step": 10050
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.2704498767852783,
      "learning_rate": 0.00017483442886083683,
      "loss": 0.7673,
      "step": 10100
    },
    {
      "epoch": 0.812,
      "grad_norm": 1.6243623495101929,
      "learning_rate": 0.0001745412319893591,
      "loss": 0.8118,
      "step": 10150
    },
    {
      "epoch": 0.816,
      "grad_norm": 1.460695505142212,
      "learning_rate": 0.00017424658592607214,
      "loss": 0.8795,
      "step": 10200
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.2373272031545639,
      "learning_rate": 0.0001739504963993313,
      "loss": 0.7851,
      "step": 10250
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.7940791845321655,
      "learning_rate": 0.0001736529691655549,
      "loss": 0.78,
      "step": 10300
    },
    {
      "epoch": 0.828,
      "grad_norm": 1.0385892391204834,
      "learning_rate": 0.00017335401000911254,
      "loss": 0.811,
      "step": 10350
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.3467884063720703,
      "learning_rate": 0.0001730536247422123,
      "loss": 0.825,
      "step": 10400
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.5690786838531494,
      "learning_rate": 0.00017275181920478822,
      "loss": 0.8555,
      "step": 10450
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.1634347438812256,
      "learning_rate": 0.0001724485992643863,
      "loss": 0.8072,
      "step": 10500
    },
    {
      "epoch": 0.844,
      "grad_norm": 1.127351999282837,
      "learning_rate": 0.00017214397081605074,
      "loss": 0.8263,
      "step": 10550
    },
    {
      "epoch": 0.848,
      "grad_norm": 1.4714469909667969,
      "learning_rate": 0.0001718379397822091,
      "loss": 0.8824,
      "step": 10600
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.2745760977268219,
      "learning_rate": 0.00017153051211255744,
      "loss": 0.7967,
      "step": 10650
    },
    {
      "epoch": 0.856,
      "grad_norm": 2.3978464603424072,
      "learning_rate": 0.00017122169378394432,
      "loss": 0.7311,
      "step": 10700
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.5721673965454102,
      "learning_rate": 0.00017091149080025492,
      "loss": 0.8242,
      "step": 10750
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.920699954032898,
      "learning_rate": 0.00017059990919229403,
      "loss": 0.8029,
      "step": 10800
    },
    {
      "epoch": 0.868,
      "grad_norm": 1.502199411392212,
      "learning_rate": 0.000170286955017669,
      "loss": 0.7765,
      "step": 10850
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.3372446298599243,
      "learning_rate": 0.00016997263436067188,
      "loss": 0.7881,
      "step": 10900
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.4501402378082275,
      "learning_rate": 0.00016965695333216125,
      "loss": 0.815,
      "step": 10950
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.3419325351715088,
      "learning_rate": 0.00016933991806944315,
      "loss": 0.768,
      "step": 11000
    },
    {
      "epoch": 0.884,
      "grad_norm": 1.9738298654556274,
      "learning_rate": 0.00016902153473615213,
      "loss": 0.8654,
      "step": 11050
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.250169038772583,
      "learning_rate": 0.00016870180952213107,
      "loss": 0.7831,
      "step": 11100
    },
    {
      "epoch": 0.892,
      "grad_norm": 1.682968258857727,
      "learning_rate": 0.0001683807486433111,
      "loss": 0.684,
      "step": 11150
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.2603853940963745,
      "learning_rate": 0.0001680583583415906,
      "loss": 0.8798,
      "step": 11200
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.33964109420776367,
      "learning_rate": 0.00016773464488471386,
      "loss": 0.7747,
      "step": 11250
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.5277403593063354,
      "learning_rate": 0.00016740961456614936,
      "loss": 0.7986,
      "step": 11300
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.8400318622589111,
      "learning_rate": 0.00016708327370496727,
      "loss": 0.8375,
      "step": 11350
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.204634428024292,
      "learning_rate": 0.00016675562864571663,
      "loss": 0.8999,
      "step": 11400
    },
    {
      "epoch": 0.916,
      "grad_norm": 1.8284354209899902,
      "learning_rate": 0.000166426685758302,
      "loss": 0.8742,
      "step": 11450
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.6564948558807373,
      "learning_rate": 0.00016609645143785974,
      "loss": 0.7726,
      "step": 11500
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.7328875660896301,
      "learning_rate": 0.0001657649321046335,
      "loss": 0.8082,
      "step": 11550
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.6156361103057861,
      "learning_rate": 0.00016543213420384945,
      "loss": 0.7598,
      "step": 11600
    },
    {
      "epoch": 0.932,
      "grad_norm": 2.051952600479126,
      "learning_rate": 0.0001650980642055911,
      "loss": 0.8406,
      "step": 11650
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.0612373352050781,
      "learning_rate": 0.00016476272860467328,
      "loss": 0.7486,
      "step": 11700
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.3454865217208862,
      "learning_rate": 0.00016442613392051616,
      "loss": 0.7749,
      "step": 11750
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.2971285879611969,
      "learning_rate": 0.0001640882866970183,
      "loss": 0.7379,
      "step": 11800
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.654829978942871,
      "learning_rate": 0.00016374919350242942,
      "loss": 0.814,
      "step": 11850
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.5190513134002686,
      "learning_rate": 0.00016340886092922277,
      "loss": 0.8059,
      "step": 11900
    },
    {
      "epoch": 0.956,
      "grad_norm": 1.3893113136291504,
      "learning_rate": 0.00016306729559396708,
      "loss": 0.7862,
      "step": 11950
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.0181379318237305,
      "learning_rate": 0.0001627245041371976,
      "loss": 0.7988,
      "step": 12000
    },
    {
      "epoch": 0.964,
      "grad_norm": 1.8384124040603638,
      "learning_rate": 0.00016238049322328735,
      "loss": 0.8356,
      "step": 12050
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.2093517780303955,
      "learning_rate": 0.00016203526954031733,
      "loss": 0.7355,
      "step": 12100
    },
    {
      "epoch": 0.972,
      "grad_norm": 1.176559567451477,
      "learning_rate": 0.00016168883979994663,
      "loss": 0.8043,
      "step": 12150
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.1152762174606323,
      "learning_rate": 0.00016134121073728178,
      "loss": 0.7934,
      "step": 12200
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.15615975856781,
      "learning_rate": 0.00016099238911074592,
      "loss": 0.8323,
      "step": 12250
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.19412223994731903,
      "learning_rate": 0.0001606423817019475,
      "loss": 0.7978,
      "step": 12300
    },
    {
      "epoch": 0.988,
      "grad_norm": 1.6698979139328003,
      "learning_rate": 0.0001602911953155483,
      "loss": 0.7821,
      "step": 12350
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.3482842445373535,
      "learning_rate": 0.0001599388367791311,
      "loss": 0.7772,
      "step": 12400
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.6071856617927551,
      "learning_rate": 0.00015958531294306703,
      "loss": 0.7992,
      "step": 12450
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0885169506072998,
      "learning_rate": 0.00015923063068038238,
      "loss": 0.7954,
      "step": 12500
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.9976464509963989,
      "learning_rate": 0.00015887479688662498,
      "loss": 0.7231,
      "step": 12550
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.2380119562149048,
      "learning_rate": 0.00015851781847973017,
      "loss": 0.675,
      "step": 12600
    },
    {
      "epoch": 1.012,
      "grad_norm": 1.118567943572998,
      "learning_rate": 0.00015815970239988617,
      "loss": 0.712,
      "step": 12650
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.756639838218689,
      "learning_rate": 0.00015780045560939926,
      "loss": 0.7565,
      "step": 12700
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.0559604167938232,
      "learning_rate": 0.00015744008509255847,
      "loss": 0.7336,
      "step": 12750
    },
    {
      "epoch": 1.024,
      "grad_norm": 2.068967342376709,
      "learning_rate": 0.00015707859785549976,
      "loss": 0.7542,
      "step": 12800
    },
    {
      "epoch": 1.028,
      "grad_norm": 1.1822625398635864,
      "learning_rate": 0.00015671600092606957,
      "loss": 0.7746,
      "step": 12850
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.9202771186828613,
      "learning_rate": 0.00015635230135368862,
      "loss": 0.7683,
      "step": 12900
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.2707793712615967,
      "learning_rate": 0.00015598750620921466,
      "loss": 0.7678,
      "step": 12950
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.9944787621498108,
      "learning_rate": 0.0001556216225848048,
      "loss": 0.7538,
      "step": 13000
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.20583830773830414,
      "learning_rate": 0.00015525465759377792,
      "loss": 0.7704,
      "step": 13050
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.4519058465957642,
      "learning_rate": 0.00015488661837047624,
      "loss": 0.7942,
      "step": 13100
    },
    {
      "epoch": 1.052,
      "grad_norm": 2.0408825874328613,
      "learning_rate": 0.00015451751207012674,
      "loss": 0.6844,
      "step": 13150
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.2264089733362198,
      "learning_rate": 0.00015414734586870172,
      "loss": 0.7109,
      "step": 13200
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.3465176522731781,
      "learning_rate": 0.00015377612696277975,
      "loss": 0.7913,
      "step": 13250
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.1009840965270996,
      "learning_rate": 0.0001534038625694055,
      "loss": 0.7148,
      "step": 13300
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.3462681174278259,
      "learning_rate": 0.0001530305599259494,
      "loss": 0.723,
      "step": 13350
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.5770235657691956,
      "learning_rate": 0.00015265622628996695,
      "loss": 0.7565,
      "step": 13400
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.6382732391357422,
      "learning_rate": 0.0001522808689390578,
      "loss": 0.7237,
      "step": 13450
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.5855664014816284,
      "learning_rate": 0.00015190449517072407,
      "loss": 0.7533,
      "step": 13500
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.7895154356956482,
      "learning_rate": 0.00015152711230222854,
      "loss": 0.7437,
      "step": 13550
    },
    {
      "epoch": 1.088,
      "grad_norm": 2.772538185119629,
      "learning_rate": 0.00015114872767045233,
      "loss": 0.7179,
      "step": 13600
    },
    {
      "epoch": 1.092,
      "grad_norm": 1.7040349245071411,
      "learning_rate": 0.00015076934863175246,
      "loss": 0.725,
      "step": 13650
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.2733752727508545,
      "learning_rate": 0.00015038898256181855,
      "loss": 0.7719,
      "step": 13700
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.300260066986084,
      "learning_rate": 0.0001500076368555297,
      "loss": 0.7509,
      "step": 13750
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.2553699314594269,
      "learning_rate": 0.0001496253189268104,
      "loss": 0.7517,
      "step": 13800
    },
    {
      "epoch": 1.108,
      "grad_norm": 1.3666965961456299,
      "learning_rate": 0.0001492420362084868,
      "loss": 0.7335,
      "step": 13850
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.20155782997608185,
      "learning_rate": 0.00014885779615214181,
      "loss": 0.7648,
      "step": 13900
    },
    {
      "epoch": 1.116,
      "grad_norm": 1.6495410203933716,
      "learning_rate": 0.00014847260622797054,
      "loss": 0.7469,
      "step": 13950
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.6937704086303711,
      "learning_rate": 0.00014808647392463485,
      "loss": 0.771,
      "step": 14000
    },
    {
      "epoch": 1.124,
      "grad_norm": 1.659922480583191,
      "learning_rate": 0.0001476994067491179,
      "loss": 0.6793,
      "step": 14050
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.3192566633224487,
      "learning_rate": 0.0001473114122265781,
      "loss": 0.7532,
      "step": 14100
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 1.466186761856079,
      "learning_rate": 0.00014692249790020288,
      "loss": 0.764,
      "step": 14150
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.3148841857910156,
      "learning_rate": 0.000146532671331062,
      "loss": 0.7806,
      "step": 14200
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.758277952671051,
      "learning_rate": 0.00014614194009796063,
      "loss": 0.7633,
      "step": 14250
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.5088104009628296,
      "learning_rate": 0.00014575031179729186,
      "loss": 0.7859,
      "step": 14300
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.6844919323921204,
      "learning_rate": 0.00014535779404288915,
      "loss": 0.7551,
      "step": 14350
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.246591329574585,
      "learning_rate": 0.00014496439446587823,
      "loss": 0.7321,
      "step": 14400
    },
    {
      "epoch": 1.156,
      "grad_norm": 1.3683743476867676,
      "learning_rate": 0.00014457012071452885,
      "loss": 0.7901,
      "step": 14450
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.1412150859832764,
      "learning_rate": 0.00014417498045410588,
      "loss": 0.7495,
      "step": 14500
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.3478381633758545,
      "learning_rate": 0.00014377898136672042,
      "loss": 0.7214,
      "step": 14550
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.6924680471420288,
      "learning_rate": 0.0001433821311511806,
      "loss": 0.7134,
      "step": 14600
    },
    {
      "epoch": 1.172,
      "grad_norm": 1.7113754749298096,
      "learning_rate": 0.00014298443752284152,
      "loss": 0.7675,
      "step": 14650
    },
    {
      "epoch": 1.176,
      "grad_norm": 1.6387418508529663,
      "learning_rate": 0.00014258590821345562,
      "loss": 0.8468,
      "step": 14700
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.4953579902648926,
      "learning_rate": 0.00014218655097102213,
      "loss": 0.7114,
      "step": 14750
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.4966493844985962,
      "learning_rate": 0.00014178637355963657,
      "loss": 0.7819,
      "step": 14800
    },
    {
      "epoch": 1.188,
      "grad_norm": 1.586800456047058,
      "learning_rate": 0.00014138538375933977,
      "loss": 0.7217,
      "step": 14850
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.4279547929763794,
      "learning_rate": 0.0001409835893659666,
      "loss": 0.7053,
      "step": 14900
    },
    {
      "epoch": 1.196,
      "grad_norm": 1.586782693862915,
      "learning_rate": 0.00014058099819099434,
      "loss": 0.7623,
      "step": 14950
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.5451070070266724,
      "learning_rate": 0.00014017761806139095,
      "loss": 0.7414,
      "step": 15000
    },
    {
      "epoch": 1.204,
      "grad_norm": 1.7088502645492554,
      "learning_rate": 0.00013977345681946284,
      "loss": 0.729,
      "step": 15050
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.5301221013069153,
      "learning_rate": 0.00013936852232270235,
      "loss": 0.7564,
      "step": 15100
    },
    {
      "epoch": 1.212,
      "grad_norm": 1.7884232997894287,
      "learning_rate": 0.0001389628224436351,
      "loss": 0.8451,
      "step": 15150
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.3007097542285919,
      "learning_rate": 0.0001385563650696668,
      "loss": 0.7625,
      "step": 15200
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.5070828199386597,
      "learning_rate": 0.00013814915810293003,
      "loss": 0.7179,
      "step": 15250
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.3013797700405121,
      "learning_rate": 0.00013774120946013054,
      "loss": 0.7905,
      "step": 15300
    },
    {
      "epoch": 1.228,
      "grad_norm": 1.0277820825576782,
      "learning_rate": 0.00013733252707239343,
      "loss": 0.7384,
      "step": 15350
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.6121586561203003,
      "learning_rate": 0.00013692311888510874,
      "loss": 0.6492,
      "step": 15400
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.244523525238037,
      "learning_rate": 0.0001365129928577773,
      "loss": 0.7304,
      "step": 15450
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.6403899192810059,
      "learning_rate": 0.0001361021569638557,
      "loss": 0.6566,
      "step": 15500
    },
    {
      "epoch": 1.244,
      "grad_norm": 1.4068081378936768,
      "learning_rate": 0.00013569061919060143,
      "loss": 0.7459,
      "step": 15550
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.387609839439392,
      "learning_rate": 0.00013527838753891745,
      "loss": 0.7556,
      "step": 15600
    },
    {
      "epoch": 1.252,
      "grad_norm": 1.5560722351074219,
      "learning_rate": 0.00013486547002319695,
      "loss": 0.7285,
      "step": 15650
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.2822256088256836,
      "learning_rate": 0.00013445187467116715,
      "loss": 0.7627,
      "step": 15700
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.4427939653396606,
      "learning_rate": 0.00013403760952373348,
      "loss": 0.7833,
      "step": 15750
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.6554374694824219,
      "learning_rate": 0.00013362268263482317,
      "loss": 0.684,
      "step": 15800
    },
    {
      "epoch": 1.268,
      "grad_norm": 1.3348811864852905,
      "learning_rate": 0.0001332071020712287,
      "loss": 0.7008,
      "step": 15850
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.7917546629905701,
      "learning_rate": 0.00013279087591245095,
      "loss": 0.7026,
      "step": 15900
    },
    {
      "epoch": 1.276,
      "grad_norm": 1.522758960723877,
      "learning_rate": 0.0001323740122505421,
      "loss": 0.7468,
      "step": 15950
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.722509503364563,
      "learning_rate": 0.00013195651918994833,
      "loss": 0.8172,
      "step": 16000
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.2210756540298462,
      "learning_rate": 0.00013153840484735234,
      "loss": 0.7556,
      "step": 16050
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.2450120449066162,
      "learning_rate": 0.00013111967735151536,
      "loss": 0.8015,
      "step": 16100
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.6564315557479858,
      "learning_rate": 0.00013070034484311932,
      "loss": 0.7646,
      "step": 16150
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.4332528114318848,
      "learning_rate": 0.0001302804154746084,
      "loss": 0.7543,
      "step": 16200
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.6785179376602173,
      "learning_rate": 0.00012985989741003072,
      "loss": 0.8114,
      "step": 16250
    },
    {
      "epoch": 1.304,
      "grad_norm": 1.6751707792282104,
      "learning_rate": 0.00012943879882487943,
      "loss": 0.7914,
      "step": 16300
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.31608256697654724,
      "learning_rate": 0.00012901712790593396,
      "loss": 0.7193,
      "step": 16350
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.0065264701843262,
      "learning_rate": 0.0001285948928511006,
      "loss": 0.7332,
      "step": 16400
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.4421634674072266,
      "learning_rate": 0.00012817210186925353,
      "loss": 0.6662,
      "step": 16450
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.9308433532714844,
      "learning_rate": 0.00012774876318007468,
      "loss": 0.7684,
      "step": 16500
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.2830469012260437,
      "learning_rate": 0.00012732488501389442,
      "loss": 0.7293,
      "step": 16550
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.554401159286499,
      "learning_rate": 0.00012690047561153125,
      "loss": 0.7304,
      "step": 16600
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.3128131031990051,
      "learning_rate": 0.00012647554322413174,
      "loss": 0.7371,
      "step": 16650
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.6018695831298828,
      "learning_rate": 0.00012605009611301013,
      "loss": 0.781,
      "step": 16700
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.386579990386963,
      "learning_rate": 0.00012562414254948742,
      "loss": 0.7945,
      "step": 16750
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.3194806575775146,
      "learning_rate": 0.0001251976908147311,
      "loss": 0.7088,
      "step": 16800
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1.4148420095443726,
      "learning_rate": 0.00012477074919959354,
      "loss": 0.768,
      "step": 16850
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.244889736175537,
      "learning_rate": 0.0001243433260044514,
      "loss": 0.7157,
      "step": 16900
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 1.737663745880127,
      "learning_rate": 0.0001239154295390437,
      "loss": 0.7462,
      "step": 16950
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.6784020066261292,
      "learning_rate": 0.00012348706812231075,
      "loss": 0.7431,
      "step": 17000
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.790999174118042,
      "learning_rate": 0.00012305825008223208,
      "loss": 0.704,
      "step": 17050
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.7120375633239746,
      "learning_rate": 0.00012262898375566466,
      "loss": 0.7826,
      "step": 17100
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.22707787156105042,
      "learning_rate": 0.0001221992774881809,
      "loss": 0.7577,
      "step": 17150
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.24608151614665985,
      "learning_rate": 0.0001217691396339062,
      "loss": 0.7849,
      "step": 17200
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.3418164551258087,
      "learning_rate": 0.00012133857855535671,
      "loss": 0.7628,
      "step": 17250
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.6251676082611084,
      "learning_rate": 0.00012090760262327665,
      "loss": 0.7472,
      "step": 17300
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.3344043493270874,
      "learning_rate": 0.00012047622021647566,
      "loss": 0.7323,
      "step": 17350
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.24828766286373138,
      "learning_rate": 0.0001200444397216658,
      "loss": 0.7311,
      "step": 17400
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.3327418863773346,
      "learning_rate": 0.00011961226953329853,
      "loss": 0.737,
      "step": 17450
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.20384174585342407,
      "learning_rate": 0.00011917971805340167,
      "loss": 0.7147,
      "step": 17500
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.6809343695640564,
      "learning_rate": 0.00011874679369141571,
      "loss": 0.7607,
      "step": 17550
    },
    {
      "epoch": 1.408,
      "grad_norm": 1.5669211149215698,
      "learning_rate": 0.00011831350486403066,
      "loss": 0.7357,
      "step": 17600
    },
    {
      "epoch": 1.412,
      "grad_norm": 1.2255433797836304,
      "learning_rate": 0.0001178798599950222,
      "loss": 0.7944,
      "step": 17650
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.6760997772216797,
      "learning_rate": 0.00011744586751508806,
      "loss": 0.7347,
      "step": 17700
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.7944185137748718,
      "learning_rate": 0.00011701153586168399,
      "loss": 0.8235,
      "step": 17750
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.4691845178604126,
      "learning_rate": 0.00011657687347885977,
      "loss": 0.7429,
      "step": 17800
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.6713278293609619,
      "learning_rate": 0.00011614188881709505,
      "loss": 0.7334,
      "step": 17850
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.612794041633606,
      "learning_rate": 0.00011570659033313511,
      "loss": 0.7447,
      "step": 17900
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.3639253079891205,
      "learning_rate": 0.00011527098648982632,
      "loss": 0.7371,
      "step": 17950
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.9603127241134644,
      "learning_rate": 0.00011483508575595175,
      "loss": 0.7825,
      "step": 18000
    },
    {
      "epoch": 1.444,
      "grad_norm": 1.3375306129455566,
      "learning_rate": 0.00011439889660606646,
      "loss": 0.6939,
      "step": 18050
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.3963803052902222,
      "learning_rate": 0.00011396242752033268,
      "loss": 0.7271,
      "step": 18100
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.8417657613754272,
      "learning_rate": 0.00011352568698435509,
      "loss": 0.7328,
      "step": 18150
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.20760397613048553,
      "learning_rate": 0.00011308868348901567,
      "loss": 0.7427,
      "step": 18200
    },
    {
      "epoch": 1.46,
      "grad_norm": 2.3450825214385986,
      "learning_rate": 0.00011265142553030883,
      "loss": 0.687,
      "step": 18250
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.6434952020645142,
      "learning_rate": 0.00011221392160917601,
      "loss": 0.7874,
      "step": 18300
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.19873130321502686,
      "learning_rate": 0.00011177618023134062,
      "loss": 0.7198,
      "step": 18350
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.032378077507019,
      "learning_rate": 0.00011133820990714254,
      "loss": 0.7221,
      "step": 18400
    },
    {
      "epoch": 1.476,
      "grad_norm": 1.3123334646224976,
      "learning_rate": 0.00011090001915137272,
      "loss": 0.7766,
      "step": 18450
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.7540221214294434,
      "learning_rate": 0.00011046161648310758,
      "loss": 0.7691,
      "step": 18500
    },
    {
      "epoch": 1.484,
      "grad_norm": 2.305863380432129,
      "learning_rate": 0.00011002301042554346,
      "loss": 0.8203,
      "step": 18550
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.2908024787902832,
      "learning_rate": 0.00010958420950583091,
      "loss": 0.766,
      "step": 18600
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.29850879311561584,
      "learning_rate": 0.00010914522225490887,
      "loss": 0.7891,
      "step": 18650
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.608527660369873,
      "learning_rate": 0.00010870605720733884,
      "loss": 0.7215,
      "step": 18700
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.8076947331428528,
      "learning_rate": 0.0001082667229011389,
      "loss": 0.7883,
      "step": 18750
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.3268853425979614,
      "learning_rate": 0.0001078272278776179,
      "loss": 0.7732,
      "step": 18800
    },
    {
      "epoch": 1.508,
      "grad_norm": 1.5839550495147705,
      "learning_rate": 0.00010738758068120914,
      "loss": 0.7428,
      "step": 18850
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.5955653190612793,
      "learning_rate": 0.00010694778985930443,
      "loss": 0.7372,
      "step": 18900
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.8254995942115784,
      "learning_rate": 0.00010650786396208785,
      "loss": 0.7541,
      "step": 18950
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.8988749980926514,
      "learning_rate": 0.00010606781154236963,
      "loss": 0.7114,
      "step": 19000
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.2124657928943634,
      "learning_rate": 0.00010562764115541967,
      "loss": 0.7791,
      "step": 19050
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.551270842552185,
      "learning_rate": 0.00010518736135880138,
      "loss": 0.8417,
      "step": 19100
    },
    {
      "epoch": 1.532,
      "grad_norm": 1.0654211044311523,
      "learning_rate": 0.0001047469807122053,
      "loss": 0.7439,
      "step": 19150
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.421156167984009,
      "learning_rate": 0.00010430650777728257,
      "loss": 0.7127,
      "step": 19200
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.572988510131836,
      "learning_rate": 0.00010386595111747857,
      "loss": 0.7408,
      "step": 19250
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.8830100297927856,
      "learning_rate": 0.00010342531929786645,
      "loss": 0.8082,
      "step": 19300
    },
    {
      "epoch": 1.548,
      "grad_norm": 1.081634759902954,
      "learning_rate": 0.00010298462088498052,
      "loss": 0.7509,
      "step": 19350
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.2041981816291809,
      "learning_rate": 0.00010254386444664985,
      "loss": 0.8438,
      "step": 19400
    },
    {
      "epoch": 1.556,
      "grad_norm": 1.246256709098816,
      "learning_rate": 0.00010210305855183148,
      "loss": 0.7692,
      "step": 19450
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.2486164569854736,
      "learning_rate": 0.00010166221177044412,
      "loss": 0.8187,
      "step": 19500
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.19673720002174377,
      "learning_rate": 0.00010122133267320121,
      "loss": 0.7426,
      "step": 19550
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.7338042855262756,
      "learning_rate": 0.00010078042983144463,
      "loss": 0.8222,
      "step": 19600
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.8778383731842041,
      "learning_rate": 0.00010033951181697768,
      "loss": 0.7588,
      "step": 19650
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.285514235496521,
      "learning_rate": 9.989858720189881e-05,
      "loss": 0.7168,
      "step": 19700
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.47810861468315125,
      "learning_rate": 9.945766455843476e-05,
      "loss": 0.7344,
      "step": 19750
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.42716681957244873,
      "learning_rate": 9.901675245877388e-05,
      "loss": 0.7332,
      "step": 19800
    },
    {
      "epoch": 1.588,
      "grad_norm": 1.7468626499176025,
      "learning_rate": 9.857585947489965e-05,
      "loss": 0.7395,
      "step": 19850
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.3119828701019287,
      "learning_rate": 9.813499417842373e-05,
      "loss": 0.7261,
      "step": 19900
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.9950352311134338,
      "learning_rate": 9.769416514041965e-05,
      "loss": 0.7653,
      "step": 19950
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.327695608139038,
      "learning_rate": 9.725338093125595e-05,
      "loss": 0.7711,
      "step": 20000
    },
    {
      "epoch": 1.604,
      "grad_norm": 1.9967050552368164,
      "learning_rate": 9.68126501204297e-05,
      "loss": 0.8037,
      "step": 20050
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.3867005109786987,
      "learning_rate": 9.637198127639977e-05,
      "loss": 0.747,
      "step": 20100
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.2068362981081009,
      "learning_rate": 9.593138296642024e-05,
      "loss": 0.7543,
      "step": 20150
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.173553228378296,
      "learning_rate": 9.549086375637407e-05,
      "loss": 0.7551,
      "step": 20200
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.0957964658737183,
      "learning_rate": 9.505043221060625e-05,
      "loss": 0.6844,
      "step": 20250
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.7394239902496338,
      "learning_rate": 9.461009689175753e-05,
      "loss": 0.7492,
      "step": 20300
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.19668030738830566,
      "learning_rate": 9.41698663605978e-05,
      "loss": 0.7381,
      "step": 20350
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.6177318096160889,
      "learning_rate": 9.372974917585981e-05,
      "loss": 0.6976,
      "step": 20400
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.5076205134391785,
      "learning_rate": 9.328975389407261e-05,
      "loss": 0.7483,
      "step": 20450
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.9085177779197693,
      "learning_rate": 9.284988906939535e-05,
      "loss": 0.7075,
      "step": 20500
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 1.6813985109329224,
      "learning_rate": 9.241016325345082e-05,
      "loss": 0.7373,
      "step": 20550
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.499572217464447,
      "learning_rate": 9.197058499515932e-05,
      "loss": 0.7472,
      "step": 20600
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1.4925471544265747,
      "learning_rate": 9.153116284057237e-05,
      "loss": 0.7373,
      "step": 20650
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1.1164584159851074,
      "learning_rate": 9.109190533270672e-05,
      "loss": 0.7155,
      "step": 20700
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.3730589151382446,
      "learning_rate": 9.065282101137798e-05,
      "loss": 0.7351,
      "step": 20750
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.549025297164917,
      "learning_rate": 9.021391841303484e-05,
      "loss": 0.8073,
      "step": 20800
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 1.2540605068206787,
      "learning_rate": 8.977520607059308e-05,
      "loss": 0.7599,
      "step": 20850
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.2544736862182617,
      "learning_rate": 8.933669251326959e-05,
      "loss": 0.7645,
      "step": 20900
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 1.0740548372268677,
      "learning_rate": 8.889838626641644e-05,
      "loss": 0.7363,
      "step": 20950
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.4239202737808228,
      "learning_rate": 8.846029585135549e-05,
      "loss": 0.7532,
      "step": 21000
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.2428124099969864,
      "learning_rate": 8.802242978521242e-05,
      "loss": 0.6595,
      "step": 21050
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.18678273260593414,
      "learning_rate": 8.758479658075117e-05,
      "loss": 0.8013,
      "step": 21100
    },
    {
      "epoch": 1.692,
      "grad_norm": 1.3034316301345825,
      "learning_rate": 8.714740474620869e-05,
      "loss": 0.7521,
      "step": 21150
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.013940453529358,
      "learning_rate": 8.671026278512911e-05,
      "loss": 0.7728,
      "step": 21200
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.3186746835708618,
      "learning_rate": 8.627337919619877e-05,
      "loss": 0.7567,
      "step": 21250
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.829970121383667,
      "learning_rate": 8.583676247308093e-05,
      "loss": 0.7828,
      "step": 21300
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.6370285749435425,
      "learning_rate": 8.540042110425048e-05,
      "loss": 0.7091,
      "step": 21350
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.3543480634689331,
      "learning_rate": 8.496436357282905e-05,
      "loss": 0.7319,
      "step": 21400
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.2164316326379776,
      "learning_rate": 8.452859835642004e-05,
      "loss": 0.7671,
      "step": 21450
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.224585771560669,
      "learning_rate": 8.409313392694389e-05,
      "loss": 0.7497,
      "step": 21500
    },
    {
      "epoch": 1.724,
      "grad_norm": 1.5044844150543213,
      "learning_rate": 8.36579787504732e-05,
      "loss": 0.8112,
      "step": 21550
    },
    {
      "epoch": 1.728,
      "grad_norm": 1.5718966722488403,
      "learning_rate": 8.322314128706823e-05,
      "loss": 0.7435,
      "step": 21600
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.32996881008148193,
      "learning_rate": 8.278862999061251e-05,
      "loss": 0.6656,
      "step": 21650
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.890823483467102,
      "learning_rate": 8.235445330864835e-05,
      "loss": 0.7944,
      "step": 21700
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.8005913496017456,
      "learning_rate": 8.192061968221274e-05,
      "loss": 0.7407,
      "step": 21750
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.2051931470632553,
      "learning_rate": 8.148713754567296e-05,
      "loss": 0.7884,
      "step": 21800
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.9832155704498291,
      "learning_rate": 8.105401532656304e-05,
      "loss": 0.7379,
      "step": 21850
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.2140648066997528,
      "learning_rate": 8.062126144541951e-05,
      "loss": 0.7091,
      "step": 21900
    },
    {
      "epoch": 1.756,
      "grad_norm": 1.4932714700698853,
      "learning_rate": 8.018888431561803e-05,
      "loss": 0.8089,
      "step": 21950
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.3404650688171387,
      "learning_rate": 7.975689234320948e-05,
      "loss": 0.7587,
      "step": 22000
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.7032501697540283,
      "learning_rate": 7.932529392675681e-05,
      "loss": 0.7583,
      "step": 22050
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.2437992095947266,
      "learning_rate": 7.889409745717167e-05,
      "loss": 0.7245,
      "step": 22100
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.6075561046600342,
      "learning_rate": 7.846331131755126e-05,
      "loss": 0.7925,
      "step": 22150
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.2026687860488892,
      "learning_rate": 7.803294388301529e-05,
      "loss": 0.7008,
      "step": 22200
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.871658980846405,
      "learning_rate": 7.760300352054327e-05,
      "loss": 0.7699,
      "step": 22250
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.47286131978034973,
      "learning_rate": 7.717349858881185e-05,
      "loss": 0.7601,
      "step": 22300
    },
    {
      "epoch": 1.788,
      "grad_norm": 1.4588068723678589,
      "learning_rate": 7.674443743803216e-05,
      "loss": 0.746,
      "step": 22350
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.3471832275390625,
      "learning_rate": 7.631582840978772e-05,
      "loss": 0.7486,
      "step": 22400
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.533102035522461,
      "learning_rate": 7.588767983687195e-05,
      "loss": 0.7798,
      "step": 22450
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.1467374563217163,
      "learning_rate": 7.54600000431264e-05,
      "loss": 0.767,
      "step": 22500
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.1181148290634155,
      "learning_rate": 7.503279734327892e-05,
      "loss": 0.7121,
      "step": 22550
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.2556784749031067,
      "learning_rate": 7.460608004278192e-05,
      "loss": 0.7506,
      "step": 22600
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.2678654193878174,
      "learning_rate": 7.417985643765083e-05,
      "loss": 0.6874,
      "step": 22650
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.1986359357833862,
      "learning_rate": 7.375413481430302e-05,
      "loss": 0.7324,
      "step": 22700
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.2445663213729858,
      "learning_rate": 7.332892344939661e-05,
      "loss": 0.7029,
      "step": 22750
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.344610333442688,
      "learning_rate": 7.290423060966947e-05,
      "loss": 0.7767,
      "step": 22800
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 2.206843137741089,
      "learning_rate": 7.248006455177857e-05,
      "loss": 0.6939,
      "step": 22850
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.249061942100525,
      "learning_rate": 7.205643352213951e-05,
      "loss": 0.8029,
      "step": 22900
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.2314731478691101,
      "learning_rate": 7.163334575676611e-05,
      "loss": 0.7416,
      "step": 22950
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.6845030784606934,
      "learning_rate": 7.121080948111046e-05,
      "loss": 0.818,
      "step": 23000
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.19844546914100647,
      "learning_rate": 7.078883290990258e-05,
      "loss": 0.7346,
      "step": 23050
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.3270955085754395,
      "learning_rate": 7.036742424699131e-05,
      "loss": 0.7769,
      "step": 23100
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 1.6127082109451294,
      "learning_rate": 6.994659168518435e-05,
      "loss": 0.7504,
      "step": 23150
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.23393838107585907,
      "learning_rate": 6.952634340608923e-05,
      "loss": 0.7512,
      "step": 23200
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.438064694404602,
      "learning_rate": 6.910668757995409e-05,
      "loss": 0.8055,
      "step": 23250
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.18907976150512695,
      "learning_rate": 6.86876323655089e-05,
      "loss": 0.7214,
      "step": 23300
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 1.2026630640029907,
      "learning_rate": 6.826918590980694e-05,
      "loss": 0.7923,
      "step": 23350
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.17391742765903473,
      "learning_rate": 6.78513563480662e-05,
      "loss": 0.7974,
      "step": 23400
    },
    {
      "epoch": 1.876,
      "grad_norm": 1.272303819656372,
      "learning_rate": 6.74341518035115e-05,
      "loss": 0.7517,
      "step": 23450
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.0447919368743896,
      "learning_rate": 6.701758038721619e-05,
      "loss": 0.8027,
      "step": 23500
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.518424928188324,
      "learning_rate": 6.660165019794485e-05,
      "loss": 0.7855,
      "step": 23550
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.1642705798149109,
      "learning_rate": 6.618636932199557e-05,
      "loss": 0.7903,
      "step": 23600
    },
    {
      "epoch": 1.892,
      "grad_norm": 1.6274757385253906,
      "learning_rate": 6.577174583304289e-05,
      "loss": 0.7521,
      "step": 23650
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.5926415324211121,
      "learning_rate": 6.535778779198069e-05,
      "loss": 0.7118,
      "step": 23700
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.9523029327392578,
      "learning_rate": 6.494450324676561e-05,
      "loss": 0.7542,
      "step": 23750
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.19023114442825317,
      "learning_rate": 6.453190023226052e-05,
      "loss": 0.7857,
      "step": 23800
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.3006070852279663,
      "learning_rate": 6.411998677007838e-05,
      "loss": 0.7188,
      "step": 23850
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.1547858715057373,
      "learning_rate": 6.370877086842615e-05,
      "loss": 0.729,
      "step": 23900
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.2679496705532074,
      "learning_rate": 6.329826052194918e-05,
      "loss": 0.7084,
      "step": 23950
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.1085736751556396,
      "learning_rate": 6.288846371157589e-05,
      "loss": 0.7205,
      "step": 24000
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.468683123588562,
      "learning_rate": 6.247938840436242e-05,
      "loss": 0.7052,
      "step": 24050
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.801142692565918,
      "learning_rate": 6.20710425533378e-05,
      "loss": 0.6824,
      "step": 24100
    },
    {
      "epoch": 1.932,
      "grad_norm": 1.134846568107605,
      "learning_rate": 6.166343409734943e-05,
      "loss": 0.7674,
      "step": 24150
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.361809492111206,
      "learning_rate": 6.125657096090858e-05,
      "loss": 0.705,
      "step": 24200
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.7139068841934204,
      "learning_rate": 6.085046105403649e-05,
      "loss": 0.7426,
      "step": 24250
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.1832175254821777,
      "learning_rate": 6.044511227211038e-05,
      "loss": 0.7411,
      "step": 24300
    },
    {
      "epoch": 1.948,
      "grad_norm": 1.570953369140625,
      "learning_rate": 6.004053249571021e-05,
      "loss": 0.7284,
      "step": 24350
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.9634497165679932,
      "learning_rate": 5.9636729590465255e-05,
      "loss": 0.7138,
      "step": 24400
    },
    {
      "epoch": 1.956,
      "grad_norm": 1.1554063558578491,
      "learning_rate": 5.9233711406901325e-05,
      "loss": 0.7747,
      "step": 24450
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.231880784034729,
      "learning_rate": 5.883148578028801e-05,
      "loss": 0.7473,
      "step": 24500
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.36060261726379395,
      "learning_rate": 5.843006053048644e-05,
      "loss": 0.7657,
      "step": 24550
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.3206781148910522,
      "learning_rate": 5.8029443461797275e-05,
      "loss": 0.7354,
      "step": 24600
    },
    {
      "epoch": 1.972,
      "grad_norm": 1.1707801818847656,
      "learning_rate": 5.762964236280885e-05,
      "loss": 0.7205,
      "step": 24650
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.2905652523040771,
      "learning_rate": 5.723066500624603e-05,
      "loss": 0.7085,
      "step": 24700
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.9856966733932495,
      "learning_rate": 5.683251914881869e-05,
      "loss": 0.7396,
      "step": 24750
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.8672142624855042,
      "learning_rate": 5.6435212531071177e-05,
      "loss": 0.7669,
      "step": 24800
    },
    {
      "epoch": 1.988,
      "grad_norm": 1.063550353050232,
      "learning_rate": 5.603875287723195e-05,
      "loss": 0.7088,
      "step": 24850
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.9650677442550659,
      "learning_rate": 5.5643147895063074e-05,
      "loss": 0.77,
      "step": 24900
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.111714482307434,
      "learning_rate": 5.524840527571051e-05,
      "loss": 0.7433,
      "step": 24950
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.14928345382213593,
      "learning_rate": 5.485453269355467e-05,
      "loss": 0.6913,
      "step": 25000
    },
    {
      "epoch": 2.004,
      "grad_norm": 1.1897367238998413,
      "learning_rate": 5.446153780606125e-05,
      "loss": 0.6928,
      "step": 25050
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.26034244894981384,
      "learning_rate": 5.406942825363215e-05,
      "loss": 0.5829,
      "step": 25100
    },
    {
      "epoch": 2.012,
      "grad_norm": 1.5003410577774048,
      "learning_rate": 5.3678211659456946e-05,
      "loss": 0.701,
      "step": 25150
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.2126910239458084,
      "learning_rate": 5.3287895629365024e-05,
      "loss": 0.6771,
      "step": 25200
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5080981254577637,
      "learning_rate": 5.2898487751677325e-05,
      "loss": 0.7049,
      "step": 25250
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.8992481231689453,
      "learning_rate": 5.2509995597059034e-05,
      "loss": 0.6885,
      "step": 25300
    },
    {
      "epoch": 2.028,
      "grad_norm": 1.3697956800460815,
      "learning_rate": 5.2122426718372165e-05,
      "loss": 0.7039,
      "step": 25350
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.23812292516231537,
      "learning_rate": 5.1735788650529104e-05,
      "loss": 0.6412,
      "step": 25400
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.19799202680587769,
      "learning_rate": 5.135008891034576e-05,
      "loss": 0.6701,
      "step": 25450
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.9774429798126221,
      "learning_rate": 5.096533499639562e-05,
      "loss": 0.6717,
      "step": 25500
    },
    {
      "epoch": 2.044,
      "grad_norm": 1.1016939878463745,
      "learning_rate": 5.058153438886386e-05,
      "loss": 0.7094,
      "step": 25550
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.3384158611297607,
      "learning_rate": 5.0198694549402e-05,
      "loss": 0.7084,
      "step": 25600
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.18995825946331024,
      "learning_rate": 4.981682292098284e-05,
      "loss": 0.6269,
      "step": 25650
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.2848252058029175,
      "learning_rate": 4.943592692775565e-05,
      "loss": 0.7005,
      "step": 25700
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.3851258754730225,
      "learning_rate": 4.905601397490196e-05,
      "loss": 0.647,
      "step": 25750
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.6516554951667786,
      "learning_rate": 4.867709144849154e-05,
      "loss": 0.6631,
      "step": 25800
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.9316348433494568,
      "learning_rate": 4.829916671533879e-05,
      "loss": 0.6739,
      "step": 25850
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.28729650378227234,
      "learning_rate": 4.7922247122859495e-05,
      "loss": 0.706,
      "step": 25900
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.8128238320350647,
      "learning_rate": 4.754633999892819e-05,
      "loss": 0.6944,
      "step": 25950
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.797752857208252,
      "learning_rate": 4.7171452651735274e-05,
      "loss": 0.7164,
      "step": 26000
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.20731866359710693,
      "learning_rate": 4.6797592369645285e-05,
      "loss": 0.6715,
      "step": 26050
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.6958164572715759,
      "learning_rate": 4.642476642105521e-05,
      "loss": 0.6849,
      "step": 26100
    },
    {
      "epoch": 2.092,
      "grad_norm": 1.3306248188018799,
      "learning_rate": 4.6052982054252947e-05,
      "loss": 0.6437,
      "step": 26150
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.3122437000274658,
      "learning_rate": 4.568224649727645e-05,
      "loss": 0.6344,
      "step": 26200
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.358180284500122,
      "learning_rate": 4.531256695777333e-05,
      "loss": 0.6309,
      "step": 26250
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.2737395763397217,
      "learning_rate": 4.494395062286073e-05,
      "loss": 0.6727,
      "step": 26300
    },
    {
      "epoch": 2.108,
      "grad_norm": 1.311318039894104,
      "learning_rate": 4.457640465898544e-05,
      "loss": 0.6968,
      "step": 26350
    },
    {
      "epoch": 2.112,
      "grad_norm": 1.5806699991226196,
      "learning_rate": 4.4209936211784566e-05,
      "loss": 0.6298,
      "step": 26400
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.18201205134391785,
      "learning_rate": 4.384455240594688e-05,
      "loss": 0.6449,
      "step": 26450
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.7052080631256104,
      "learning_rate": 4.348026034507402e-05,
      "loss": 0.7142,
      "step": 26500
    },
    {
      "epoch": 2.124,
      "grad_norm": 1.1494405269622803,
      "learning_rate": 4.311706711154252e-05,
      "loss": 0.6856,
      "step": 26550
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.8430196642875671,
      "learning_rate": 4.275497976636596e-05,
      "loss": 0.6084,
      "step": 26600
    },
    {
      "epoch": 2.132,
      "grad_norm": 1.2949244976043701,
      "learning_rate": 4.239400534905803e-05,
      "loss": 0.7108,
      "step": 26650
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.2310570329427719,
      "learning_rate": 4.2034150877495316e-05,
      "loss": 0.66,
      "step": 26700
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.16906188428401947,
      "learning_rate": 4.1675423347781015e-05,
      "loss": 0.6947,
      "step": 26750
    },
    {
      "epoch": 2.144,
      "grad_norm": 1.1796939373016357,
      "learning_rate": 4.1317829734108946e-05,
      "loss": 0.6247,
      "step": 26800
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.1980106085538864,
      "learning_rate": 4.0961376988627886e-05,
      "loss": 0.6764,
      "step": 26850
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.2375468909740448,
      "learning_rate": 4.060607204130648e-05,
      "loss": 0.6639,
      "step": 26900
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.5490866303443909,
      "learning_rate": 4.025192179979843e-05,
      "loss": 0.6657,
      "step": 26950
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5477752089500427,
      "learning_rate": 3.98989331493083e-05,
      "loss": 0.6428,
      "step": 27000
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.9946629405021667,
      "learning_rate": 3.95471129524576e-05,
      "loss": 0.6813,
      "step": 27050
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.1600115299224854,
      "learning_rate": 3.9196468049151325e-05,
      "loss": 0.7278,
      "step": 27100
    },
    {
      "epoch": 2.172,
      "grad_norm": 1.3139476776123047,
      "learning_rate": 3.8847005256445055e-05,
      "loss": 0.7092,
      "step": 27150
    },
    {
      "epoch": 2.176,
      "grad_norm": 1.921970009803772,
      "learning_rate": 3.849873136841249e-05,
      "loss": 0.6182,
      "step": 27200
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.3239564895629883,
      "learning_rate": 3.8151653156013055e-05,
      "loss": 0.6626,
      "step": 27250
    },
    {
      "epoch": 2.184,
      "grad_norm": 1.3820875883102417,
      "learning_rate": 3.7805777366960646e-05,
      "loss": 0.6954,
      "step": 27300
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.4557289779186249,
      "learning_rate": 3.746111072559216e-05,
      "loss": 0.6207,
      "step": 27350
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.0766446590423584,
      "learning_rate": 3.7117659932737035e-05,
      "loss": 0.6555,
      "step": 27400
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.7420526146888733,
      "learning_rate": 3.67754316655866e-05,
      "loss": 0.6993,
      "step": 27450
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.4907901287078857,
      "learning_rate": 3.643443257756459e-05,
      "loss": 0.6868,
      "step": 27500
    },
    {
      "epoch": 2.204,
      "grad_norm": 1.4356929063796997,
      "learning_rate": 3.609466929819774e-05,
      "loss": 0.6992,
      "step": 27550
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.3169984519481659,
      "learning_rate": 3.5756148432986755e-05,
      "loss": 0.6374,
      "step": 27600
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.191598579287529,
      "learning_rate": 3.5418876563277935e-05,
      "loss": 0.6546,
      "step": 27650
    },
    {
      "epoch": 2.216,
      "grad_norm": 1.4272236824035645,
      "learning_rate": 3.5082860246135306e-05,
      "loss": 0.6595,
      "step": 27700
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.3582954406738281,
      "learning_rate": 3.474810601421317e-05,
      "loss": 0.6634,
      "step": 27750
    },
    {
      "epoch": 2.224,
      "grad_norm": 1.4403005838394165,
      "learning_rate": 3.4414620375628946e-05,
      "loss": 0.6489,
      "step": 27800
    },
    {
      "epoch": 2.228,
      "grad_norm": 1.051845908164978,
      "learning_rate": 3.4082409813836624e-05,
      "loss": 0.6878,
      "step": 27850
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.8473628163337708,
      "learning_rate": 3.375148078750104e-05,
      "loss": 0.6535,
      "step": 27900
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.5236810445785522,
      "learning_rate": 3.342183973037193e-05,
      "loss": 0.6726,
      "step": 27950
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.23723790049552917,
      "learning_rate": 3.3093493051159074e-05,
      "loss": 0.6493,
      "step": 28000
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.8282837271690369,
      "learning_rate": 3.2766447133407486e-05,
      "loss": 0.6588,
      "step": 28050
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.7578852772712708,
      "learning_rate": 3.2440708335373674e-05,
      "loss": 0.6772,
      "step": 28100
    },
    {
      "epoch": 2.252,
      "grad_norm": 1.2277657985687256,
      "learning_rate": 3.2116282989901656e-05,
      "loss": 0.6963,
      "step": 28150
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 1.6208910942077637,
      "learning_rate": 3.1793177404300037e-05,
      "loss": 0.7041,
      "step": 28200
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.0159300565719604,
      "learning_rate": 3.147139786021932e-05,
      "loss": 0.6235,
      "step": 28250
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.455710768699646,
      "learning_rate": 3.115095061352982e-05,
      "loss": 0.6406,
      "step": 28300
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.30492228269577026,
      "learning_rate": 3.0831841894199995e-05,
      "loss": 0.6529,
      "step": 28350
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.3192579448223114,
      "learning_rate": 3.0514077906175353e-05,
      "loss": 0.6734,
      "step": 28400
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.1750524342060089,
      "learning_rate": 3.0197664827257942e-05,
      "loss": 0.6814,
      "step": 28450
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.3378884196281433,
      "learning_rate": 2.9882608808985934e-05,
      "loss": 0.7013,
      "step": 28500
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.8801370859146118,
      "learning_rate": 2.9568915976514376e-05,
      "loss": 0.6825,
      "step": 28550
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.2407807558774948,
      "learning_rate": 2.925659242849593e-05,
      "loss": 0.629,
      "step": 28600
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.2963739037513733,
      "learning_rate": 2.8945644236962434e-05,
      "loss": 0.6843,
      "step": 28650
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.22324323654174805,
      "learning_rate": 2.8636077447206568e-05,
      "loss": 0.6494,
      "step": 28700
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.1624093055725098,
      "learning_rate": 2.832789807766465e-05,
      "loss": 0.6581,
      "step": 28750
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.40309950709342957,
      "learning_rate": 2.802111211979952e-05,
      "loss": 0.6309,
      "step": 28800
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.47317206859588623,
      "learning_rate": 2.7715725537983996e-05,
      "loss": 0.676,
      "step": 28850
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.46035730838775635,
      "learning_rate": 2.7411744269384888e-05,
      "loss": 0.72,
      "step": 28900
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.35986950993537903,
      "learning_rate": 2.7109174223847687e-05,
      "loss": 0.6673,
      "step": 28950
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.20184843242168427,
      "learning_rate": 2.6808021283781705e-05,
      "loss": 0.6588,
      "step": 29000
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.5203787684440613,
      "learning_rate": 2.6508291304045553e-05,
      "loss": 0.6827,
      "step": 29050
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.6574400663375854,
      "learning_rate": 2.6209990111833283e-05,
      "loss": 0.7163,
      "step": 29100
    },
    {
      "epoch": 2.332,
      "grad_norm": 1.2627499103546143,
      "learning_rate": 2.591312350656142e-05,
      "loss": 0.6552,
      "step": 29150
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.6090571880340576,
      "learning_rate": 2.5617697259755845e-05,
      "loss": 0.7062,
      "step": 29200
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.6536394357681274,
      "learning_rate": 2.5323717114939792e-05,
      "loss": 0.6753,
      "step": 29250
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.0119978189468384,
      "learning_rate": 2.503118878752212e-05,
      "loss": 0.6212,
      "step": 29300
    },
    {
      "epoch": 2.348,
      "grad_norm": 1.280535101890564,
      "learning_rate": 2.4740117964686217e-05,
      "loss": 0.6507,
      "step": 29350
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.5312000513076782,
      "learning_rate": 2.445051030527942e-05,
      "loss": 0.6484,
      "step": 29400
    },
    {
      "epoch": 2.356,
      "grad_norm": 1.6174224615097046,
      "learning_rate": 2.416237143970299e-05,
      "loss": 0.671,
      "step": 29450
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.1980282962322235,
      "learning_rate": 2.3875706969802715e-05,
      "loss": 0.6425,
      "step": 29500
    },
    {
      "epoch": 2.364,
      "grad_norm": 1.3311598300933838,
      "learning_rate": 2.359052246875988e-05,
      "loss": 0.6477,
      "step": 29550
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.6958693861961365,
      "learning_rate": 2.3306823480983053e-05,
      "loss": 0.6863,
      "step": 29600
    },
    {
      "epoch": 2.372,
      "grad_norm": 1.4414231777191162,
      "learning_rate": 2.3024615522000182e-05,
      "loss": 0.6811,
      "step": 29650
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.17209622263908386,
      "learning_rate": 2.274390407835142e-05,
      "loss": 0.6954,
      "step": 29700
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.4427505731582642,
      "learning_rate": 2.246469460748246e-05,
      "loss": 0.6831,
      "step": 29750
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.9717238545417786,
      "learning_rate": 2.2186992537638406e-05,
      "loss": 0.6257,
      "step": 29800
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.7780838012695312,
      "learning_rate": 2.191080326775825e-05,
      "loss": 0.6925,
      "step": 29850
    },
    {
      "epoch": 2.392,
      "grad_norm": 1.1376731395721436,
      "learning_rate": 2.1636132167370016e-05,
      "loss": 0.6926,
      "step": 29900
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.36727961897850037,
      "learning_rate": 2.136298457648608e-05,
      "loss": 0.6316,
      "step": 29950
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.17905868589878082,
      "learning_rate": 2.1091365805499663e-05,
      "loss": 0.6589,
      "step": 30000
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.7858572602272034,
      "learning_rate": 2.0821281135081448e-05,
      "loss": 0.697,
      "step": 30050
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.5523383617401123,
      "learning_rate": 2.0552735816076996e-05,
      "loss": 0.7203,
      "step": 30100
    },
    {
      "epoch": 2.412,
      "grad_norm": 1.1930524110794067,
      "learning_rate": 2.0285735069404443e-05,
      "loss": 0.6631,
      "step": 30150
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.2987321615219116,
      "learning_rate": 2.0020284085953256e-05,
      "loss": 0.6835,
      "step": 30200
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.8223375082015991,
      "learning_rate": 1.975638802648324e-05,
      "loss": 0.6606,
      "step": 30250
    },
    {
      "epoch": 2.424,
      "grad_norm": 1.239160180091858,
      "learning_rate": 1.9494052021524167e-05,
      "loss": 0.6346,
      "step": 30300
    },
    {
      "epoch": 2.428,
      "grad_norm": 1.3860121965408325,
      "learning_rate": 1.923328117127591e-05,
      "loss": 0.6899,
      "step": 30350
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.20267701148986816,
      "learning_rate": 1.8974080545509598e-05,
      "loss": 0.6567,
      "step": 30400
    },
    {
      "epoch": 2.436,
      "grad_norm": 1.4664255380630493,
      "learning_rate": 1.8716455183468796e-05,
      "loss": 0.6666,
      "step": 30450
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.3259809017181396,
      "learning_rate": 1.8460410093771597e-05,
      "loss": 0.6712,
      "step": 30500
    },
    {
      "epoch": 2.444,
      "grad_norm": 1.316626787185669,
      "learning_rate": 1.8205950254313275e-05,
      "loss": 0.7315,
      "step": 30550
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.7875133156776428,
      "learning_rate": 1.7953080612169516e-05,
      "loss": 0.6253,
      "step": 30600
    },
    {
      "epoch": 2.452,
      "grad_norm": 1.6043171882629395,
      "learning_rate": 1.7701806083500193e-05,
      "loss": 0.6053,
      "step": 30650
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.19703447818756104,
      "learning_rate": 1.745213155345382e-05,
      "loss": 0.6729,
      "step": 30700
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.24424006044864655,
      "learning_rate": 1.7204061876072574e-05,
      "loss": 0.6465,
      "step": 30750
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.2322579622268677,
      "learning_rate": 1.6957601874197915e-05,
      "loss": 0.6698,
      "step": 30800
    },
    {
      "epoch": 2.468,
      "grad_norm": 1.0632878541946411,
      "learning_rate": 1.6712756339376856e-05,
      "loss": 0.6614,
      "step": 30850
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.9114142656326294,
      "learning_rate": 1.646953003176873e-05,
      "loss": 0.6662,
      "step": 30900
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.6775410175323486,
      "learning_rate": 1.6227927680052766e-05,
      "loss": 0.6097,
      "step": 30950
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.24029846489429474,
      "learning_rate": 1.5987953981336033e-05,
      "loss": 0.7054,
      "step": 31000
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.18615713715553284,
      "learning_rate": 1.5749613601062197e-05,
      "loss": 0.6719,
      "step": 31050
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.8024306893348694,
      "learning_rate": 1.5512911172920775e-05,
      "loss": 0.6237,
      "step": 31100
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.8782767057418823,
      "learning_rate": 1.5277851298757175e-05,
      "loss": 0.6742,
      "step": 31150
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.3464122414588928,
      "learning_rate": 1.5044438548482953e-05,
      "loss": 0.6374,
      "step": 31200
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.01498544216156,
      "learning_rate": 1.4812677459987278e-05,
      "loss": 0.6501,
      "step": 31250
    },
    {
      "epoch": 2.504,
      "grad_norm": 1.8368374109268188,
      "learning_rate": 1.4582572539048478e-05,
      "loss": 0.6717,
      "step": 31300
    },
    {
      "epoch": 2.508,
      "grad_norm": 1.1307635307312012,
      "learning_rate": 1.435412825924668e-05,
      "loss": 0.625,
      "step": 31350
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.979859471321106,
      "learning_rate": 1.4127349061876494e-05,
      "loss": 0.6872,
      "step": 31400
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.23866404592990875,
      "learning_rate": 1.390223935586098e-05,
      "loss": 0.6732,
      "step": 31450
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.0486758947372437,
      "learning_rate": 1.367880351766584e-05,
      "loss": 0.6821,
      "step": 31500
    },
    {
      "epoch": 2.524,
      "grad_norm": 1.4035804271697998,
      "learning_rate": 1.3457045891214236e-05,
      "loss": 0.7153,
      "step": 31550
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.18304307758808136,
      "learning_rate": 1.3236970787802495e-05,
      "loss": 0.6894,
      "step": 31600
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.22110137343406677,
      "learning_rate": 1.3018582486016052e-05,
      "loss": 0.6474,
      "step": 31650
    },
    {
      "epoch": 2.536,
      "grad_norm": 1.2857567071914673,
      "learning_rate": 1.2801885231646615e-05,
      "loss": 0.6577,
      "step": 31700
    },
    {
      "epoch": 2.54,
      "grad_norm": 3.25986909866333,
      "learning_rate": 1.2586883237609326e-05,
      "loss": 0.7006,
      "step": 31750
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.4990395307540894,
      "learning_rate": 1.2373580683860964e-05,
      "loss": 0.6868,
      "step": 31800
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.835969865322113,
      "learning_rate": 1.2161981717318682e-05,
      "loss": 0.6961,
      "step": 31850
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.739778459072113,
      "learning_rate": 1.1952090451779386e-05,
      "loss": 0.6738,
      "step": 31900
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.7512692809104919,
      "learning_rate": 1.1743910967839732e-05,
      "loss": 0.627,
      "step": 31950
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.349826693534851,
      "learning_rate": 1.153744731281684e-05,
      "loss": 0.6581,
      "step": 32000
    },
    {
      "epoch": 2.564,
      "grad_norm": 1.112396478652954,
      "learning_rate": 1.1332703500669551e-05,
      "loss": 0.6914,
      "step": 32050
    },
    {
      "epoch": 2.568,
      "grad_norm": 1.438561201095581,
      "learning_rate": 1.112968351192043e-05,
      "loss": 0.6353,
      "step": 32100
    },
    {
      "epoch": 2.572,
      "grad_norm": 1.1216849088668823,
      "learning_rate": 1.0928391293578344e-05,
      "loss": 0.75,
      "step": 32150
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.6320927143096924,
      "learning_rate": 1.0728830759061781e-05,
      "loss": 0.6376,
      "step": 32200
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.20415212213993073,
      "learning_rate": 1.0531005788122705e-05,
      "loss": 0.6078,
      "step": 32250
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.8747718334197998,
      "learning_rate": 1.0334920226771172e-05,
      "loss": 0.6757,
      "step": 32300
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.9974994659423828,
      "learning_rate": 1.014057788720052e-05,
      "loss": 0.6626,
      "step": 32350
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.6233751773834229,
      "learning_rate": 9.947982547713353e-06,
      "loss": 0.6775,
      "step": 32400
    },
    {
      "epoch": 2.596,
      "grad_norm": 1.23455810546875,
      "learning_rate": 9.7571379526479e-06,
      "loss": 0.676,
      "step": 32450
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.1430566310882568,
      "learning_rate": 9.568047812305382e-06,
      "loss": 0.6213,
      "step": 32500
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.706781268119812,
      "learning_rate": 9.380715802877827e-06,
      "loss": 0.6496,
      "step": 32550
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.5557529926300049,
      "learning_rate": 9.19514556637665e-06,
      "loss": 0.6437,
      "step": 32600
    },
    {
      "epoch": 2.612,
      "grad_norm": 1.6847580671310425,
      "learning_rate": 9.011340710561666e-06,
      "loss": 0.7247,
      "step": 32650
    },
    {
      "epoch": 2.616,
      "grad_norm": 2.0218160152435303,
      "learning_rate": 8.829304808871163e-06,
      "loss": 0.6803,
      "step": 32700
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.4823530912399292,
      "learning_rate": 8.649041400352376e-06,
      "loss": 0.6742,
      "step": 32750
    },
    {
      "epoch": 2.624,
      "grad_norm": 1.6307358741760254,
      "learning_rate": 8.470553989592566e-06,
      "loss": 0.6519,
      "step": 32800
    },
    {
      "epoch": 2.628,
      "grad_norm": 1.055701494216919,
      "learning_rate": 8.293846046651055e-06,
      "loss": 0.6841,
      "step": 32850
    },
    {
      "epoch": 2.632,
      "grad_norm": 1.3106586933135986,
      "learning_rate": 8.118921006991564e-06,
      "loss": 0.6715,
      "step": 32900
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.6204774975776672,
      "learning_rate": 7.945782271415669e-06,
      "loss": 0.6438,
      "step": 32950
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.448155164718628,
      "learning_rate": 7.774433205996478e-06,
      "loss": 0.6341,
      "step": 33000
    },
    {
      "epoch": 2.644,
      "grad_norm": 1.1852730512619019,
      "learning_rate": 7.604877142013267e-06,
      "loss": 0.6577,
      "step": 33050
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.43263891339302063,
      "learning_rate": 7.437117375886726e-06,
      "loss": 0.647,
      "step": 33100
    },
    {
      "epoch": 2.652,
      "grad_norm": 1.5444366931915283,
      "learning_rate": 7.271157169114862e-06,
      "loss": 0.6755,
      "step": 33150
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.1687220335006714,
      "learning_rate": 7.106999748209575e-06,
      "loss": 0.6428,
      "step": 33200
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.292839765548706,
      "learning_rate": 6.944648304633938e-06,
      "loss": 0.6987,
      "step": 33250
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.375769019126892,
      "learning_rate": 6.784105994740164e-06,
      "loss": 0.6317,
      "step": 33300
    },
    {
      "epoch": 2.668,
      "grad_norm": 1.3188508749008179,
      "learning_rate": 6.625375939708223e-06,
      "loss": 0.6845,
      "step": 33350
    },
    {
      "epoch": 2.672,
      "grad_norm": 1.3932727575302124,
      "learning_rate": 6.468461225485178e-06,
      "loss": 0.6146,
      "step": 33400
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.9373946189880371,
      "learning_rate": 6.3133649027251825e-06,
      "loss": 0.6472,
      "step": 33450
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.269225001335144,
      "learning_rate": 6.1600899867301665e-06,
      "loss": 0.6684,
      "step": 33500
    },
    {
      "epoch": 2.684,
      "grad_norm": 1.359729528427124,
      "learning_rate": 6.008639457391196e-06,
      "loss": 0.6722,
      "step": 33550
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 1.5298489332199097,
      "learning_rate": 5.8590162591306006e-06,
      "loss": 0.6682,
      "step": 33600
    },
    {
      "epoch": 2.692,
      "grad_norm": 1.1198782920837402,
      "learning_rate": 5.7112233008446815e-06,
      "loss": 0.648,
      "step": 33650
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.18459439277648926,
      "learning_rate": 5.565263455847136e-06,
      "loss": 0.6532,
      "step": 33700
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.1112052202224731,
      "learning_rate": 5.4211395618132595e-06,
      "loss": 0.6933,
      "step": 33750
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.19356182217597961,
      "learning_rate": 5.278854420724732e-06,
      "loss": 0.7018,
      "step": 33800
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.17415113747119904,
      "learning_rate": 5.138410798815197e-06,
      "loss": 0.6979,
      "step": 33850
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 1.2982473373413086,
      "learning_rate": 4.999811426516354e-06,
      "loss": 0.617,
      "step": 33900
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.43805843591690063,
      "learning_rate": 4.863058998405035e-06,
      "loss": 0.6369,
      "step": 33950
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.5196326971054077,
      "learning_rate": 4.728156173150711e-06,
      "loss": 0.6434,
      "step": 34000
    },
    {
      "epoch": 2.724,
      "grad_norm": 1.3843202590942383,
      "learning_rate": 4.595105573463865e-06,
      "loss": 0.6372,
      "step": 34050
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.9029992818832397,
      "learning_rate": 4.463909786044973e-06,
      "loss": 0.6746,
      "step": 34100
    },
    {
      "epoch": 2.732,
      "grad_norm": 1.4581477642059326,
      "learning_rate": 4.3345713615341565e-06,
      "loss": 0.6661,
      "step": 34150
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.6070986390113831,
      "learning_rate": 4.207092814461755e-06,
      "loss": 0.6735,
      "step": 34200
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.9140332341194153,
      "learning_rate": 4.081476623199254e-06,
      "loss": 0.6148,
      "step": 34250
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.20569923520088196,
      "learning_rate": 3.957725229911236e-06,
      "loss": 0.6404,
      "step": 34300
    },
    {
      "epoch": 2.748,
      "grad_norm": 1.074812650680542,
      "learning_rate": 3.8358410405078034e-06,
      "loss": 0.6613,
      "step": 34350
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.2413581609725952,
      "learning_rate": 3.7158264245979237e-06,
      "loss": 0.6208,
      "step": 34400
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.7882599234580994,
      "learning_rate": 3.5976837154432453e-06,
      "loss": 0.6865,
      "step": 34450
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.5129550695419312,
      "learning_rate": 3.4814152099127752e-06,
      "loss": 0.6354,
      "step": 34500
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 1.6389796733856201,
      "learning_rate": 3.3670231684382547e-06,
      "loss": 0.692,
      "step": 34550
    },
    {
      "epoch": 2.768,
      "grad_norm": 1.6330292224884033,
      "learning_rate": 3.2545098149701637e-06,
      "loss": 0.6651,
      "step": 34600
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.8788347244262695,
      "learning_rate": 3.1438773369345196e-06,
      "loss": 0.6549,
      "step": 34650
    },
    {
      "epoch": 2.776,
      "grad_norm": 1.0986666679382324,
      "learning_rate": 3.0351278851903586e-06,
      "loss": 0.6683,
      "step": 34700
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.8118098974227905,
      "learning_rate": 2.928263573987855e-06,
      "loss": 0.7311,
      "step": 34750
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.5553733706474304,
      "learning_rate": 2.823286480927312e-06,
      "loss": 0.6636,
      "step": 34800
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1.5387210845947266,
      "learning_rate": 2.7201986469186925e-06,
      "loss": 0.752,
      "step": 34850
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.7966338992118835,
      "learning_rate": 2.6190020761419633e-06,
      "loss": 0.6057,
      "step": 34900
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 1.0721455812454224,
      "learning_rate": 2.519698736008158e-06,
      "loss": 0.6745,
      "step": 34950
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.9398546814918518,
      "learning_rate": 2.4222905571210986e-06,
      "loss": 0.6662,
      "step": 35000
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 1.2005137205123901,
      "learning_rate": 2.326779433239845e-06,
      "loss": 0.7074,
      "step": 35050
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.989229679107666,
      "learning_rate": 2.2331672212419495e-06,
      "loss": 0.6489,
      "step": 35100
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 1.0318710803985596,
      "learning_rate": 2.141455741087273e-06,
      "loss": 0.6681,
      "step": 35150
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.6994969248771667,
      "learning_rate": 2.0516467757826452e-06,
      "loss": 0.6217,
      "step": 35200
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.1608581840991974,
      "learning_rate": 1.9637420713471854e-06,
      "loss": 0.6795,
      "step": 35250
    },
    {
      "epoch": 2.824,
      "grad_norm": 1.0754424333572388,
      "learning_rate": 1.8777433367783815e-06,
      "loss": 0.6382,
      "step": 35300
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.21591900289058685,
      "learning_rate": 1.7936522440188618e-06,
      "loss": 0.6807,
      "step": 35350
    },
    {
      "epoch": 2.832,
      "grad_norm": 1.287395715713501,
      "learning_rate": 1.7114704279238114e-06,
      "loss": 0.6623,
      "step": 35400
    },
    {
      "epoch": 2.836,
      "grad_norm": 1.5655686855316162,
      "learning_rate": 1.6311994862293289e-06,
      "loss": 0.7119,
      "step": 35450
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.0588302612304688,
      "learning_rate": 1.5528409795212417e-06,
      "loss": 0.6819,
      "step": 35500
    },
    {
      "epoch": 2.844,
      "grad_norm": 1.36591637134552,
      "learning_rate": 1.4763964312048406e-06,
      "loss": 0.6643,
      "step": 35550
    },
    {
      "epoch": 2.848,
      "grad_norm": 1.1051928997039795,
      "learning_rate": 1.4018673274751925e-06,
      "loss": 0.6933,
      "step": 35600
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.8856328129768372,
      "learning_rate": 1.3292551172883417e-06,
      "loss": 0.6963,
      "step": 35650
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.4786970317363739,
      "learning_rate": 1.2585612123330536e-06,
      "loss": 0.6512,
      "step": 35700
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.0559872388839722,
      "learning_rate": 1.189786987003405e-06,
      "loss": 0.6176,
      "step": 35750
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.6145832538604736,
      "learning_rate": 1.1229337783720594e-06,
      "loss": 0.6775,
      "step": 35800
    },
    {
      "epoch": 2.868,
      "grad_norm": 1.4302866458892822,
      "learning_rate": 1.0580028861642888e-06,
      "loss": 0.6418,
      "step": 35850
    },
    {
      "epoch": 2.872,
      "grad_norm": 1.287274718284607,
      "learning_rate": 9.94995572732682e-07,
      "loss": 0.696,
      "step": 35900
    },
    {
      "epoch": 2.876,
      "grad_norm": 1.1663964986801147,
      "learning_rate": 9.339130630326098e-07,
      "loss": 0.6117,
      "step": 35950
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.21561512351036072,
      "learning_rate": 8.747565445983985e-07,
      "loss": 0.6971,
      "step": 36000
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.9794255495071411,
      "learning_rate": 8.175271675202934e-07,
      "loss": 0.6988,
      "step": 36050
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.5687258243560791,
      "learning_rate": 7.622260444220208e-07,
      "loss": 0.6652,
      "step": 36100
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.2145683765411377,
      "learning_rate": 7.088542504392282e-07,
      "loss": 0.6211,
      "step": 36150
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.6845428943634033,
      "learning_rate": 6.57412823198511e-07,
      "loss": 0.6623,
      "step": 36200
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.4195044040679932,
      "learning_rate": 6.079027627973299e-07,
      "loss": 0.7116,
      "step": 36250
    },
    {
      "epoch": 2.904,
      "grad_norm": 1.2071571350097656,
      "learning_rate": 5.603250317844811e-07,
      "loss": 0.6544,
      "step": 36300
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.6682128310203552,
      "learning_rate": 5.146805551414335e-07,
      "loss": 0.62,
      "step": 36350
    },
    {
      "epoch": 2.912,
      "grad_norm": 1.1571708917617798,
      "learning_rate": 4.7097022026433293e-07,
      "loss": 0.6726,
      "step": 36400
    },
    {
      "epoch": 2.916,
      "grad_norm": 1.2571096420288086,
      "learning_rate": 4.291948769467369e-07,
      "loss": 0.6361,
      "step": 36450
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.9623495936393738,
      "learning_rate": 3.893553373631176e-07,
      "loss": 0.6761,
      "step": 36500
    },
    {
      "epoch": 2.924,
      "grad_norm": 1.1272873878479004,
      "learning_rate": 3.5145237605305193e-07,
      "loss": 0.6424,
      "step": 36550
    },
    {
      "epoch": 2.928,
      "grad_norm": 1.1042070388793945,
      "learning_rate": 3.154867299061781e-07,
      "loss": 0.6857,
      "step": 36600
    },
    {
      "epoch": 2.932,
      "grad_norm": 1.1658062934875488,
      "learning_rate": 2.8145909814785155e-07,
      "loss": 0.6437,
      "step": 36650
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.7515258193016052,
      "learning_rate": 2.4937014232556675e-07,
      "loss": 0.6763,
      "step": 36700
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.1536511182785034,
      "learning_rate": 2.1922048629609004e-07,
      "loss": 0.6515,
      "step": 36750
    },
    {
      "epoch": 2.944,
      "grad_norm": 1.3995232582092285,
      "learning_rate": 1.9101071621333567e-07,
      "loss": 0.6869,
      "step": 36800
    },
    {
      "epoch": 2.948,
      "grad_norm": 1.001887559890747,
      "learning_rate": 1.6474138051695287e-07,
      "loss": 0.6183,
      "step": 36850
    },
    {
      "epoch": 2.952,
      "grad_norm": 1.6266345977783203,
      "learning_rate": 1.4041298992168994e-07,
      "loss": 0.7107,
      "step": 36900
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.20092011988162994,
      "learning_rate": 1.1802601740744656e-07,
      "loss": 0.654,
      "step": 36950
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.16483503580093384,
      "learning_rate": 9.75808982100701e-08,
      "loss": 0.6825,
      "step": 37000
    }
  ],
  "logging_steps": 50,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0045668710520422e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
